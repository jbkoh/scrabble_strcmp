{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n"
     ]
    }
   ],
   "source": [
    "#essential libraries\n",
    "\n",
    "from google_drive import gdrive\n",
    "\n",
    "from matplotlib.pyplot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "import matplotlib\n",
    "reload(matplotlib)\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter, defaultdict, OrderedDict, deque\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy, copy\n",
    "import random\n",
    "import rosetta\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "import os\n",
    "import gc\n",
    "import linecache\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import URIRef\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy import spatial\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy import stats\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "from scipy.spatial.distance import cosine as cosine_similarity\n",
    "\n",
    "from divergence import gau_js as js_divergence\n",
    "import building_tokenizer as toker\n",
    "import brick_parser\n",
    "reload(brick_parser)\n",
    "from brick_parser import tagList, tagsetList, equipTagsetList, pointTagsetList, locationTagsetList,\\\n",
    "equalDict, pointTagList, equipTagList, locationTagList, equipPointDict\n",
    "subTagListDict = dict([('point', pointTagList),\n",
    "                          ('equip', equipTagList),\n",
    "                          ('location', locationTagList)\n",
    "                         ])\n",
    "subTagsetListDict = dict([('point', pointTagsetList),\n",
    "                          ('equip', equipTagsetList),\n",
    "                          ('location', locationTagsetList)\n",
    "                         ])\n",
    "\n",
    "#from cmu_parser import cmu_building_parse\n",
    "\n",
    "debugFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workerNum = 7\n",
    "coverageNum = 150\n",
    "mpFlag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_fig(fig, name, dpi=400):\n",
    "\tpp = PdfPages(name)\n",
    "\tpp.savefig(fig, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\tpp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_dict_with_best_n_from_dict(srcDict, n):\n",
    "    idxCnt = 0\n",
    "    sortedValueList = sorted(srcDict.values(), reverse=True)[0:n]\n",
    "    chosenItemList = list()\n",
    "    \n",
    "    for score in sortedValueList:\n",
    "        for key, val in srcDict.items():\n",
    "            if idxCnt>=n:\n",
    "                break\n",
    "            if val==score:\n",
    "                chosenItemList.append((key,val))\n",
    "                idxCnt += 1\n",
    "        if idxCnt>=n:\n",
    "                break\n",
    "    return OrderedDict(chosenItemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from sys import getsizeof, stderr\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "try:\n",
    "    from reprlib import repr\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def total_size(o, handlers={}, verbose=False):\n",
    "    \"\"\" Returns the approximate memory footprint an object and all of its contents.\n",
    "\n",
    "    Automatically finds the contents of the following builtin containers and\n",
    "    their subclasses:  tuple, list, deque, dict, set and frozenset.\n",
    "    To search other containers, add handlers to iterate over their contents:\n",
    "\n",
    "        handlers = {SomeContainerClass: iter,\n",
    "                    OtherContainerClass: OtherContainerClass.get_elements}\n",
    "\n",
    "    \"\"\"\n",
    "    dict_handler = lambda d: chain.from_iterable(d.items())\n",
    "    all_handlers = {tuple: iter,\n",
    "                    list: iter,\n",
    "                    deque: iter,\n",
    "                    dict: dict_handler,\n",
    "                    set: iter,\n",
    "                    frozenset: iter,\n",
    "                   }\n",
    "    all_handlers.update(handlers)     # user handlers take precedence\n",
    "    seen = set()                      # track which object id's have already been seen\n",
    "    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n",
    "\n",
    "    def sizeof(o):\n",
    "        if id(o) in seen:       # do not double count the same object\n",
    "            return 0\n",
    "        seen.add(id(o))\n",
    "        s = getsizeof(o, default_size)\n",
    "\n",
    "        if verbose:\n",
    "            print(s, type(o), repr(o))#, file=stderr)\n",
    "\n",
    "        for typ, handler in all_handlers.items():\n",
    "            if isinstance(o, typ):\n",
    "                s += sum(map(sizeof, handler(o)))\n",
    "                break\n",
    "        return s\n",
    "\n",
    "    return sizeof(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_equip_points(equip, g):\n",
    "    pointList = list()\n",
    "    hasPointRef = URIRef(uriPrefix+'hasPoint')\n",
    "    if equip=='vav':\n",
    "        equipRef = URIRef(uriPrefix+'VAV')\n",
    "    elif equip=='ahu':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='vfd':\n",
    "        equipRef = URIRef(uriPrefix+'VFD')\n",
    "    elif equip=='chilled_water_pump':\n",
    "        equipRef = URIRef(uriPrefix+'Chilled_Water_Pump')\n",
    "    elif equip=='hot_water_pump':\n",
    "        equipRef = URIRef(uriPrefix+'Hot_Water_Pump')\n",
    "    elif equip=='crac':\n",
    "        equipRef = URIRef(uriPrefix+'CRAC')\n",
    "    elif equip=='exhaust_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='return_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='supply_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    #elif equip=='return_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Return_Fan')\n",
    "    #elif equip=='exhaust_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Exhaust_Fan')\n",
    "    #elif equip=='supply_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Supply_Fan')\n",
    "    else:\n",
    "        print (\"equip name %s is not in the schema\" % equip)\n",
    "        assert(False)\n",
    "    for superClass in g.objects(equipRef,RDFS.subClassOf):\n",
    "        #print triple\n",
    "        #for tri in g.triples((triple, None, None)):\n",
    "        if (superClass, OWL.onProperty, hasPointRef) in g:\n",
    "            for point in g.objects(superClass, OWL.someValuesFrom):\n",
    "                pointList.append(point.decode().split(\"#\")[1].lower().split('_'))\n",
    "    return pointList\n",
    "\n",
    "def equip_ref_to_type(equipRef):\n",
    "    if equipRef==None:\n",
    "        return None\n",
    "    equipRefList = equipRef.split('_')\n",
    "    equipTypeList = list()\n",
    "    for word in equipRefList:\n",
    "        numList = re.findall('\\d+', word)\n",
    "        if len(numList)==0:\n",
    "            equipTypeList.append(word)\n",
    "    return str('_'.join(equipTypeList))\n",
    "\n",
    "def equip_ref_to_ref(equipRef):\n",
    "    if equipRef==None:\n",
    "        return None\n",
    "    equipRefList = equipRef.split('_')\n",
    "    equipTypeList = list()\n",
    "    for word in equipRefList:\n",
    "        numList = re.findall('\\d+', word)\n",
    "        if len(numList)!=0:\n",
    "            return word\n",
    "    return '9999'\n",
    "\n",
    "def get_alpha_word(word):\n",
    "    alphaWordList = re.findall('[a-zA-Z]+', word)\n",
    "    if len(alphaWordList)>0:\n",
    "        alphaWord = alphaWordList[0]\n",
    "        return alphaWord\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_num_word(word):\n",
    "    wordList = re.findall('[b]?\\d+[abc]?\\_?\\d*', word)\n",
    "    if len(wordList)>0:\n",
    "        foundWord = wordList[0]\n",
    "        return foundWord\n",
    "    else:\n",
    "        return '9999'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentenceTagsetsCombination:\n",
    "    totalWeightDict = None\n",
    "    sentence = None\n",
    "    \n",
    "    def determine_word_belongs_to_sentence(self, word, tagList):\n",
    "        #closestTagList = scoreDictDict[word].keys()[0].split()\n",
    "        #return sum([1.0 if tag in tagList else 0.0 for tag in closestTagList]) / float(len(closestTagList)) >=0.2\n",
    "        subScoreDict = pick_dict_with_best_n_from_dict(scoreDictDict[word], 3)\n",
    "        totalScore = 0\n",
    "        for tagset, score in subScoreDict.items():\n",
    "            for tag in tagset.split():\n",
    "                if tag in tagList:\n",
    "                    totalScore += score / float(len(tagset.split()))\n",
    "        #if word=='meter':\n",
    "        #    print totalScore\n",
    "        totalScore /= sum(subScoreDict.values())\n",
    "        #return score>=0.2\n",
    "        \n",
    "        return totalScore>=0.2\n",
    "    \n",
    "    def __init__(self, sentence, subTagListDict, candSet=None):\n",
    "        self.candSet = candSet\n",
    "        self.scoreHistory = [0]\n",
    "        self.sentence = sentence\n",
    "        wordScoreDictDict = dict()\n",
    "        for tagsetType in subTagListDict.keys()+['dummy']:\n",
    "            wordScoreDictDict[tagsetType] = dict()\n",
    "        \n",
    "        for word in sentence:\n",
    "            if len(word)<=1 or '' in scoreDictDict[word].keys():\n",
    "                wordScoreDictDict['dummy'][word] = 1.0\n",
    "                for tagsetType in subTagListDict.keys():\n",
    "                    wordScoreDictDict[tagsetType][word] = 0.0\n",
    "                continue\n",
    "            \n",
    "            wordOccurrenceDict = dict()\n",
    "            for tagsetType, tagList in subTagListDict.items():\n",
    "                wordOccurrenceDict[tagsetType] = 1.0 if self.determine_word_belongs_to_sentence(word, tagList) else 0.0\n",
    "            totalOcc = sum(wordOccurrenceDict.values())\n",
    "            \n",
    "            #if word=='meter': ##########\n",
    "            #    print wordOccurrenceDict\n",
    "            \n",
    "            if totalOcc==0:\n",
    "                wordScoreDictDict['dummy'][word] = 1.0\n",
    "                for tagsetType in wordOccurrenceDict.keys():\n",
    "                    wordScoreDictDict[tagsetType][word] = 0.0\n",
    "            else:\n",
    "                wordScoreDictDict['dummy'][word]= 0.0\n",
    "                for tagsetType, wordOccurrence in wordOccurrenceDict.items():\n",
    "                    wordScoreDictDict[tagsetType][word] = wordOccurrence / totalOcc\n",
    "        \n",
    "        self.totalWeightDict = wordScoreDictDict\n",
    "        \n",
    "    def set_cand_set(self, candSet, score):\n",
    "        selfTagsetTypes = set(self.totalWeightDict.keys())\n",
    "        selfTagsetTypes.remove('dummy')\n",
    "        \n",
    "        assert(set(candSet.keys())==selfTagsetTypes)\n",
    "        self.candSet = candSet\n",
    "        self.scoreHistory.append(score)\n",
    "        \n",
    "    def set_score(self, score):\n",
    "        self.scoreHistory.append(score)\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.scoreHistory[-1]\n",
    "    \n",
    "    def get_cand_set(self):\n",
    "        return self.candSet\n",
    "        \n",
    "    def set_sentence(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def get_sentence(self):\n",
    "        return self.sentence\n",
    "    \n",
    "    def get_weight_dict(self, tagsetType):\n",
    "        return self.totalWeightDict[tagsetType]\n",
    "    \n",
    "    def get_total_weight_dict(self):\n",
    "        return self.totalWeightDict\n",
    "    \n",
    "    def set_total_weight_dict(self, newWeightDict):\n",
    "        aWeightDict = self.totalWeightDict['dummy']\n",
    "        assert(len(newWeightDict.keys())==len(self.totalWeightDict.keys()))\n",
    "        for word in aWeightDict.keys():\n",
    "            assert(np.allclose(1.0, sum([weightDict[word] for weightDict in newWeightDict.values()])))\n",
    "        self.totalWeightDict = newWeightDict\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubSentenceTagsetPair:\n",
    "    localScoreDict = dict() # similar to scoreDictDict but only contains words existing in the sentence\n",
    "    sentence = list()\n",
    "    targetTagset = ''\n",
    "    origSentence = list()\n",
    "    predLable = ''\n",
    "    score = 0 \n",
    "    direction = ''\n",
    "    scoreHistory = []\n",
    "    \n",
    "    def __init__(self, origSentence, localScoreDict, targetTagset, score=0):\n",
    "        self.origSentence = origSentence\n",
    "        self.sentence = origSentence\n",
    "        self.localScoreDict = localScoreDict\n",
    "        self.score = score\n",
    "        self.scoreHistory = [score]\n",
    "        self.targetTagset = targetTagset\n",
    "    \n",
    "    def append_score_history(self, score):\n",
    "        self.scoreHistory.append(score)\n",
    "    def get_score_history(self):\n",
    "        return self.scoreHistory\n",
    "    def set_score_dict(self, localScoreDict):\n",
    "        self.localScoreDict = localScoreDict\n",
    "    def set_direction(self, flag):\n",
    "        if not (flag=='+' or flag=='-'):\n",
    "            print 'wrong direction flag'\n",
    "            assert(False)\n",
    "        else:\n",
    "            self.direction = flag\n",
    "    def get_direction(self):\n",
    "        return self.direction\n",
    "    def get_score_dict(self):\n",
    "        return self.localScoreDict\n",
    "    def set_sentence(self, sentence):\n",
    "        self.sentence = sentence\n",
    "    def get_sentence(self):\n",
    "        return self.sentence\n",
    "    def set_score(self, score):\n",
    "        self.score = score\n",
    "    def get_score(self):\n",
    "        return self.score\n",
    "    def get_target_tagset(self):\n",
    "        return self.targetTagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import building_tokenizer\n",
    "reload(building_tokenizer)\n",
    "import building_tokenizer as toker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildingName = 'ebu3b'\n",
    "notUcsdBuildings = ['ghc']\n",
    "\n",
    "tokenType = 'NoNumber'\n",
    "\n",
    "if not buildingName in notUcsdBuildings:\n",
    "    naeDict = dict()\n",
    "    naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "    naeDict['ap_m'] = ['514', '513','604']\n",
    "    naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "    naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "    naeList = naeDict[buildingName]\n",
    "\n",
    "    labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "    with open(labeledFile, 'rb') as fp:\n",
    "        #truthDF = pd.read_excel(fp)\n",
    "        truthDF = pd.DataFrame.from_csv(fp)\n",
    "        #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "    wordFeatFile = 'data/wordfeat_'+buildingName+'.pkl'\n",
    "\n",
    "    tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "    bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "    for val in Counter(unitMap.keys()).values():\n",
    "        if val>1:\n",
    "            \"Unit map file ERROR\"\n",
    "            assert(False)\n",
    "            \n",
    "            \n",
    "    trueDF = pd.DataFrame.from_csv('metadata/'+buildingName+'_sensor_types_location.csv')\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, bacnettypeList, wordList = \\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "\n",
    "    origSensorDF = deepcopy(sensorDF)\n",
    "    origNameList = deepcopy(nameList)\n",
    "    origJcinameList = deepcopy(jcinameList)\n",
    "    origDescList = deepcopy(descList)\n",
    "    origUnitList = deepcopy(unitList)\n",
    "    origBacnettypeList = deepcopy(bacnettypeList)\n",
    "    origWordList = deepcopy(wordList)\n",
    "\n",
    "    with open('data/'+buildingName+'_str_score_dict.pkl', 'rb') as fp:\n",
    "        scoreDictDict = pickle.load(fp)\n",
    "\n",
    "    ### If a word is exactly matched with another, fix it.\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        if word in scoreDict.keys():\n",
    "            scoreDictDict[word] = {word:1}\n",
    "    \n",
    "    _, rawNameList, rawJcinameList, rawDescList, _, _, _ = toker.structure_metadata(buildingName=buildingName, tokenType='AlphaAndNum', \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "    \n",
    "    \n",
    "else: \n",
    "    filename = 'metadata/'+buildingName+'_sensor_types_location.csv'\n",
    "    #   filename = 'metadata/%s_sensor_types_location.csv'%buildingName\n",
    "    df = pd.read_csv(filename)\n",
    "    sentenceList = list()\n",
    "    \n",
    "    \n",
    "    for raw in df['bas_raw'].tolist():\n",
    "        sentenceList.append(toker.tokenize(tokenType, raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select useful indices only\n",
    "'''\n",
    "srcidList = sensorDF.index.to_series().tolist()\n",
    "idxList = list()\n",
    "addedTypeList = list()\n",
    "cnt = 0\n",
    "for srcid, row in trueDF.iterrows():\n",
    "    pointType = row['Schema Label']\n",
    "    if pointType not in addedTypeList:\n",
    "        addedTypeList.append(pointType)\n",
    "        i = srcidList.index(srcid)\n",
    "        idxList.append(i)\n",
    "        cnt +=1 \n",
    "        if cnt>=100:\n",
    "            break\n",
    "'''\n",
    "idxList = range(0,coverageNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listIndexFunc = lambda l, i: [l[ii] for ii in i]\n",
    "sensorDF = origSensorDF.iloc[idxList]\n",
    "nameList = listIndexFunc(origNameList, idxList)\n",
    "jcinameList = listIndexFunc(origJcinameList, idxList)\n",
    "descList = listIndexFunc(origDescList, idxList)\n",
    "unitList = listIndexFunc(origUnitList, idxList)\n",
    "bacnettypeList = listIndexFunc(origBacnettypeList, idxList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nameList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_max_score_dict(scoreDictDict):\n",
    "    maxScoreDict = dict()\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        maxScoreDict[word] = max(scoreDict.values())\n",
    "    return maxScoreDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Human Input\n",
    "#scoreDictDict['ebu'] = {'building':1}\n",
    "\n",
    "scoreDictDict['ebu'] = {'':1}\n",
    "scoreDictDict['vma'] = {'':1}\n",
    "scoreDictDict['nae'] = {'':1}\n",
    "scoreDictDict['meter'] = {'meter': 1}\n",
    "#scoreDictDict['ap'] = {'':1}\n",
    "#scoreDictDict['trunk'] = {'':1}\n",
    "\n",
    "scoreDictDict['rm'] = {'room':1}\n",
    "#scoreDictDict['actual'] = {'effective':1}\n",
    "#scoreDictDict['percent'] = {'':1}\n",
    "\n",
    "maxScoreDict = calc_max_score_dict(scoreDictDict)\n",
    "\n",
    "origScoreDictDict = deepcopy(scoreDictDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manual_def(word, trueTagset):\n",
    "    scoreDict = scoreDictDict[word]\n",
    "    scoreDictDict[word] = dict((tagset,1) if turTagset==tagset else (tagset,0) for tagset in scoreDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Initialization sentence separation\n",
    "#### We need equip sentence\n",
    "### We need type sentence, equip sentence, location sentence\n",
    "\n",
    "# Init no meaning sentence\n",
    "sentenceList = list()\n",
    "adder = lambda a,b:a+b\n",
    "splitter = lambda s:s.split()\n",
    "sentenceList = [reduce(adder, map(splitter, dataList))\\\n",
    "                #for dataList in zip(nameList, jcinameList, descList, unitList, bacnettypeList)]\n",
    "                for dataList in zip(nameList, jcinameList, unitList, bacnettypeList)]\n",
    "                #for dataList in zip(nameList, jcinameList, unitList)]\n",
    "                #for dataList in zip(nameList, jcinameList)]\n",
    "rawSentenceList = [reduce(adder, map(splitter, dataList))\\\n",
    "                #for dataList in zip(rawNameList, rawJcinameList, rawDescList, unitList, bacnettypeList)]\n",
    "                for dataList in zip(rawNameList, rawJcinameList, unitList, bacnettypeList)]\n",
    "                #for dataList in zip(rawNameList, rawJcinameList, unitList)]\n",
    "                #for dataList in zip(rawNameList, rawJcinameList)]\n",
    "\n",
    "# Make type, equip, location sentences\n",
    "equipSentenceList = list()\n",
    "pointSentenceList = list()\n",
    "locationSentenceList = list()\n",
    "\n",
    "for sentence in sentenceList:\n",
    "    equipSentence = list()\n",
    "    pointSentence = list()\n",
    "    locationSentence = list()\n",
    "    for word in sentence:\n",
    "        closestTagList = scoreDictDict[word].keys()[0].split()\n",
    "        if len(closestTagList)==0:\n",
    "            continue\n",
    "        if sum([1 if tag in equipTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            equipSentence.append(word)\n",
    "        if sum([1 if tag in pointTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            pointSentence.append(word)\n",
    "        if sum([1 if tag in locationTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            locationSentence.append(word)\n",
    "    equipSentenceList.append(equipSentence)\n",
    "    pointSentenceList.append(pointSentence)\n",
    "    locationSentenceList.append(locationSentence)\n",
    "\n",
    "#assert(len(trueDF)==len(pointSentenceList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_tfidf_sub(cntDict, idfDict):\n",
    "    tfidfList = list()\n",
    "    maxCnt = max(cntDict.values())\n",
    "    for tag, idf in idfDict.items():\n",
    "        if not tag in cntDict.keys():\n",
    "            tfidf = 0\n",
    "        else:\n",
    "            tfidf = (0.5 + 0.5 * cntDict[tag] / maxCnt)* idf\n",
    "        tfidfList.append(tfidf)\n",
    "    return np.asarray(tfidfList)\n",
    "\n",
    "def sentence_tagset_score_segmented_tfidf(sentence, targetTagset):\n",
    "    targetTagset = targetTagset.split('_')\n",
    "    compDataLen = 5\n",
    "    sentenceCntDict = defaultdict(float)\n",
    "    \n",
    "    for word in sentence:\n",
    "        totalWordScore = 1.0/len(sentence)\n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:compDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values())) #* len(sentence)\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTagList = candTagset.split()\n",
    "            candTagScore = score / localScoreSum / len(candTagList)\n",
    "            for candTag in candTagList:\n",
    "                sentenceCntDict[candTag] += candTagScore\n",
    "                \n",
    "    tagsetCntDict = defaultdict(float)\n",
    "    #avgCnt = np.mean(sentenceCntDict.values())\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] += 1.0/len(targetTagset)\n",
    "        \n",
    "    entireCntDict = OrderedDict(sentenceCntDict)\n",
    "    for tag in targetTagset:\n",
    "        if tag not in sentenceCntDict.keys():\n",
    "            entireCntDict[tag] = tagsetCntDict[tag]\n",
    "    \n",
    "    entireCntDict = OrderedDict(entireCntDict)\n",
    "    totalCnt = sum(entireCntDict.values())\n",
    "    idfValues = [np.log(totalCnt/cnt) for cnt in entireCntDict.values()]\n",
    "    idfDict = OrderedDict()\n",
    "    for tag, idf in zip(entireCntDict.keys(), idfValues):\n",
    "        idfDict[tag]  = idf\n",
    "    \n",
    "    sentenceTfidf = calc_tfidf_sub(sentenceCntDict, idfDict)\n",
    "    tagsetTfidf = calc_tfidf_sub(tagsetCntDict, idfDict)\n",
    "    return 1 - cosine_similarity(sentenceTfidf, tagsetTfidf)\n",
    "\n",
    "#sentence_tagset_score = sentence_tagset_score_segmented_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maybe need log-scale compensation of redundant tags or words\n",
    "\n",
    "needDataLen =12\n",
    "\n",
    "def sentence_tagset_score_word_and_sentence_together(sentence, targetTagset, scoreDictDict=scoreDictDict):\n",
    "    origSentence = sentence\n",
    "    if type(targetTagset)==str or type(targetTagset)==unicode:\n",
    "        targetTagset = targetTagset.split('_')\n",
    "    \n",
    "    totalScore = 0 \n",
    "    notUsedWordCntDict = dict()\n",
    "    for word in sentence:\n",
    "        notUsedWordCntDict[word] = 0\n",
    "        \n",
    "    tagsetCntDict = dict()\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:needDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values()))\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTags = candTagset.split()\n",
    "            for candTag in candTags:\n",
    "                candTagScore = score / localScoreSum / len(candTags)\n",
    "                if candTag in targetTagset:\n",
    "                    tagsetCntDict[candTag] += candTagScore\n",
    "                else:\n",
    "                    notUsedWordCntDict[word] += candTagScore\n",
    "    tagsetScoreDict = dict()\n",
    "    for word, cnt in tagsetCntDict.items():\n",
    "        tagsetScoreDict[word] = np.log(1+cnt)\n",
    "    return (len(sentence)-sum(notUsedWordCntDict.values()))/len(sentence) * sum(tagsetScoreDict.values()) / len(targetTagset)\n",
    "    \n",
    "sentence_tagset_score = sentence_tagset_score_word_and_sentence_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_tagset_score_with_weight_inside(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, \\\n",
    "                                             maxScoreDict=maxScoreDict, totalScore=0, sentenceLen=None):\n",
    "    if type(targetTagset)==str or type(targetTagset)==unicode:\n",
    "        targetTagset = targetTagset.split('_')\n",
    "    \n",
    "    #TODO: Choose between those two.\n",
    "    if sentenceLen==None:\n",
    "        sentenceLen = len(sentence)\n",
    "    \n",
    "    totalScoreForTagset = 0\n",
    "    for word in sentence:\n",
    "        totalScoreForTagset += weightDict[word]\n",
    "    \n",
    "    try:\n",
    "        assert_almost_equal(totalScoreForTagset,0)\n",
    "        return 0, 0, 0    \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    for word in deepcopy(sentence):\n",
    "        if len(word)<=1:\n",
    "            sentence.remove(word)\n",
    "    \n",
    "    notUsedWordCntDict = dict()\n",
    "    for word in sentence:\n",
    "        notUsedWordCntDict[word] = 0\n",
    "        \n",
    "    tagsetCntDict = dict()\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] = 0\n",
    "        \n",
    "    sysWordUsageDict = defaultdict(float)\n",
    "    \n",
    "    for word in sentence:\n",
    "        wordScore = weightDict[word] * max(scoreDictDict[word].values()) / maxScoreDict[word]\n",
    "        \n",
    "        if wordScore==0:\n",
    "            continue\n",
    "            \n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:needDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values()))\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTags = candTagset.split()\n",
    "            for candTag in candTags:\n",
    "                candTagScore = score / localScoreSum / len(candTags) * wordScore\n",
    "                if candTag in targetTagset:\n",
    "                    tagsetCntDict[candTag] += candTagScore\n",
    "                else:\n",
    "                    notUsedWordCntDict[word] += candTagScore\n",
    "    tagsetScoreDict = dict()\n",
    "    for tag, cnt in tagsetCntDict.items():\n",
    "        tagsetScoreDict[tag] = np.log(1+cnt)\n",
    "    #for tag, cnt in tagsetCntDict.items():\n",
    "    #    tagsetScoreDict[tag] = cnt / (totalScoreForTagset/float(len(targetTagset)))\n",
    "    \n",
    "    \n",
    "    notUsedWordScoreDict = dict()\n",
    "    for word, cnt in notUsedWordCntDict.items():\n",
    "        notUsedWordScoreDict[word] = np.log(1+cnt)\n",
    "    #print \"notusedscore: \", notUsedWordScoreDict['chwp']\n",
    "        \n",
    "    \n",
    "    \n",
    "    sentenceUsage = sum(tagsetCntDict.values()) / float(sentenceLen)\n",
    "    tagsetCoverage = sum(tagsetScoreDict.values()) / (len(targetTagset)*np.log(1+totalScoreForTagset/float(len(targetTagset))))\n",
    "    #tagsetCoverage = sum(tagsetCntDict.values()) / totalScoreForTagset\n",
    "    candScore = sentenceUsage  * tagsetCoverage\n",
    "    \n",
    "    #print \"---\", targetTagset\n",
    "    #print \"totalScoreForTagset\", totalScoreForTagset\n",
    "    #print \"len(targetTagset)\", len(targetTagset)\n",
    "    #print \"---\"\n",
    "    \n",
    "    \n",
    "    #print sentenceUsage\n",
    "    #print tagsetCoverage\n",
    "    #pp.pprint(tagsetScoreDict)\n",
    "    \n",
    "    \n",
    "    return candScore, sentenceUsage, tagsetCoverage#, notUsedWordCntDict\n",
    "    \n",
    "    \n",
    "def sentence_tagset_score_with_weight(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict, totalScore=0):\n",
    "    candScore, _, _ = sentence_tagset_score_with_weight_inside(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict, totalScore=0)\n",
    "    return candScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_score_altogether_in_sentence(comb, candDict, scoreDictDict):\n",
    "    sentence = comb.get_sentence()\n",
    "    #TODO: Check which is good between below two lines\n",
    "    #sentenceTotalScore = len(sentence) - sum(comb.get_weight_dict('dummy').values())\n",
    "    sentenceTotalScore = len(sentence)\n",
    "    sentenceUsageSum = 0\n",
    "    tagsetCoverageList = list()\n",
    "    weightSumList = list()\n",
    "    candScoreSum = 0\n",
    "    #TODO: Choose to include this or not\n",
    "    #sentenceLen = len(sentence) - sum(comb.get_weight_dict('dummy').values())\n",
    "    sentenceLen = None\n",
    "    for tagsetType, cand in candDict.items():\n",
    "        # TODO: Return to the original\n",
    "        candScore, sentenceUsage, tagsetCoverage = sentence_tagset_score_with_weight_inside(sentence, cand, \\\n",
    "                                        comb.get_weight_dict(tagsetType), scoreDictDict, sentenceLen=sentenceLen)\n",
    "        sentenceUsageSum += sentenceUsage\n",
    "        tagsetCoverageList.append(tagsetCoverage)\n",
    "        weightSumList.append(sum(comb.get_weight_dict(tagsetType).values())) \n",
    "        candScoreSum += candScore\n",
    "        #print cand, candScore, sentenceUsage, tagsetCoverage\n",
    "    #sentenceUsage = sentenceUsageSum / sentenceTotalScore\n",
    "    #totalTagsetCoverage = sum([cov*weight for cov, weight in zip(tagsetCoverageList, weightSumList)])/float(sum(weightSumList))\n",
    "    return candScoreSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "extractSubDict = lambda origDict, tagsetTypeList: \\\n",
    "            dict((tagsetType, subTagList) for tagsetType, subTagList in origDict.items() if tagsetType in tagsetTypeList)\n",
    "subTagListDictList = [extractSubDict(subTagListDict, ['point']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'equip']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'equip', 'location']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'location'])\n",
    "                      ]\n",
    "                      \n",
    "scoreDictDict = dict()\n",
    "\n",
    "for sentence in sentenceList[0:coverageNum]:\n",
    "#for sentence in sentenceList[2:3]:\n",
    "    for word in sentence:\n",
    "        if word not in scoreDictDict.keys():\n",
    "            scoreDictDict[word] = origScoreDictDict[word]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_comb((sentence, subTagListDictList, scoreDictDict, reduceCandNum)):\n",
    "    combList = list()\n",
    "    for tagListDict in subTagListDictList:\n",
    "        tagTypeList = tagListDict.keys()\n",
    "        comb = SentenceTagsetsCombination(sentence, tagListDict)\n",
    "        sentenceTotalScore = len(comb.get_sentence()) - sum(comb.get_weight_dict('dummy').values())\n",
    "        weightDictDict = comb.get_total_weight_dict()\n",
    "        candDictList = list()\n",
    "        multipleTagsetScoreDict = defaultdict(dict)\n",
    "        for tagsetType, weightDict in weightDictDict.items():\n",
    "            if tagsetType=='dummy':\n",
    "                continue\n",
    "            tagsetList = subTagsetListDict[tagsetType]\n",
    "            for tagset in tagsetList:\n",
    "                #multipleTagsetScoreDict[tagsetType][tagset] = sentence_tagset_score_with_weight(sentence, \\\n",
    "#                                                                                                tagset, weightDict)\n",
    "                multipleTagsetScoreDict[tagsetType][tagset] = calc_score_altogether_in_sentence(comb, {tagsetType:tagset}, \\\n",
    "                                                                                                scoreDictDict)\n",
    "        multipleTagsetScoreDict = dict(multipleTagsetScoreDict)\n",
    "        \n",
    "        pointItemList = list()\n",
    "        equipPointItemList = list()\n",
    "        \n",
    "        pointCandNum = 5 - reduceCandNum\n",
    "        if pointCandNum<1:\n",
    "            pointCandNum = 1\n",
    "        \n",
    "        pointEquipCandNum = 5 - reduceCandNum\n",
    "        if pointEquipCandNum<2:\n",
    "            pointEquipCandNum = 2\n",
    "        locationCandNum = 3 - reduceCandNum\n",
    "        if locationCandNum <2:\n",
    "            locationcandNum = 2\n",
    "        \n",
    "        chosenPointTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['point'], pointCandNum)\n",
    "        #chosenItemList \n",
    "        pointItemList += [({'point':pointTagset}, score) for pointTagset, score in chosenPointTagsetDict.items()]\n",
    "        \n",
    "        equipPointItemList = list()\n",
    "        \n",
    "        if 'equip' in tagTypeList:\n",
    "            chosenEquipTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['equip'], pointEquipCandNum)\n",
    "            for equipTagset, equipScore in chosenEquipTagsetDict.items():\n",
    "                for pointTagset, pointScore in chosenPointTagsetDict.items():\n",
    "                    if pointTagset in equipPointDict[equipTagset]:\n",
    "                        candDict = {'point': pointTagset, 'equip':equipTagset}\n",
    "                        equipPointItemList.append((candDict, calc_score_altogether_in_sentence(comb, candDict, scoreDictDict)))\n",
    "        '''\n",
    "        if 'equip' in tagTypeList:\n",
    "            chosenEquipTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['equip'], 4 - reduceCandNum)\n",
    "            for equipTagset, equipScore in chosenEquipTagsetDict.items():\n",
    "                for pointTagset, pointScore in chosenPointTagsetDict.items():\n",
    "                    candDict = {'point': pointTagset, 'equip':equipTagset}\n",
    "                    equipPointItemList.append((candDict, calc_score_altogether_in_sentence(comb, candDict, scoreDictDict)))\n",
    "        '''\n",
    "        \n",
    "        if 'location' in tagTypeList:\n",
    "            chosenLocationTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['location'], locationCandNum)\n",
    "            locationEquipPointItemList = list()\n",
    "            locationPointItemList = list()\n",
    "            for locationTagset, locationScore in chosenLocationTagsetDict.items():\n",
    "                if 'equip' in tagTypeList:\n",
    "                    for item in equipPointItemList:\n",
    "                        item[0]['location'] = locationTagset\n",
    "                        locationEquipPointItemList.append((deepcopy(item[0]), item[1]+locationScore))\n",
    "                else:\n",
    "                    for item in pointItemList:\n",
    "                        item[0]['location'] = locationTagset\n",
    "                        locationPointItemList.append((deepcopy(item[0]), item[1]+locationScore))\n",
    "                    \n",
    "                    \n",
    "        if 'equip' in tagTypeList:\n",
    "            if 'location' in tagTypeList:\n",
    "                chosenItemList = locationEquipPointItemList\n",
    "            else:\n",
    "                chosenItemList = equipPointItemList\n",
    "        elif 'location' in tagTypeList:\n",
    "            chosenItemList = locationPointItemList\n",
    "        else:\n",
    "            chosenItemList = pointItemList\n",
    "        for item in chosenItemList:\n",
    "            tempComb = deepcopy(comb)\n",
    "            tempComb.set_cand_set(item[0], item[1])\n",
    "            combList.append(tempComb)\n",
    "    return combList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535369433333 minutes\n"
     ]
    }
   ],
   "source": [
    "beginTime =  datetime.now()\n",
    "if mpFlag:\n",
    "    pool = multiprocessing.Pool(workerNum)\n",
    "    combListList = pool.map(init_comb, zip(sentenceList, repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "    pool.terminate()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "else:\n",
    "    #combListList = map(init_comb, zip(sentenceList, repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "    combListList = list()\n",
    "    for i, sentence in enumerate(sentenceList):\n",
    "        combListList.append(init_comb((sentence, subTagListDictList, scoreDictDict, 0)))\n",
    "    pass\n",
    "combListDict = dict((i,combList) for i, combList in enumerate(combListList))\n",
    "endTime = datetime.now()\n",
    "\n",
    "origCombListDict = deepcopy(combListDict)\n",
    "\n",
    "print (endTime-beginTime).total_seconds()/60, 'minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_comb_cand_dict_score(combListDict, scoreDictDict):\n",
    "    for idx, combList in combListDict.items():\n",
    "        for comb in combList:\n",
    "            score = calc_score_altogether_in_sentence(comb, comb.get_cand_set(), scoreDictDict)\n",
    "            comb.set_score(score)\n",
    "    return combListDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.817978713623763]\n"
     ]
    }
   ],
   "source": [
    "def calc_building_score(combListDict):\n",
    "    buildingScore = 0\n",
    "    scoreGetter = lambda x: x.get_score()\n",
    "    for idx, combList in combListDict.items():\n",
    "        scoreList = map(scoreGetter,combList)\n",
    "        scoreSum = sum(scoreList)\n",
    "        buildingScore += sum([score*score/scoreSum for score in scoreList])\n",
    "    return buildingScore\n",
    "\n",
    "buildingScoreList = [calc_building_score(origCombListDict)]\n",
    "print buildingScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_word_meaning2((combListDict, word, scoreDictDict)):\n",
    "    scoreDict = scoreDictDict[word]\n",
    "    if len(scoreDict)<=1:\n",
    "        return None\n",
    "    baseCombListDict = dict()\n",
    "    for i, combList in combListDict.items():\n",
    "        if word in combList[0].get_sentence():\n",
    "            baseCombListDict[i] = deepcopy(combList)\n",
    "    if len(baseCombListDict)<1:\n",
    "        return None\n",
    "    baseBuildingScore = calc_building_score(baseCombListDict)\n",
    "    \n",
    "    removeCandList = scoreDict.keys()\n",
    "    updatedCombListDictList = list()\n",
    "    for removeCand in removeCandList:\n",
    "        updatedScoreDictDict = deepcopy(scoreDictDict)\n",
    "        del updatedScoreDictDict[word][removeCand]\n",
    "        combListDict = deepcopy(baseCombListDict)\n",
    "        updatedCombListDictList.append(update_comb_cand_dict_score(combListDict, scoreDictDict=updatedScoreDictDict))\n",
    "    updatedScoreList = list()\n",
    "    for updatedCombListDict in updatedCombListDictList:\n",
    "        updatedScoreList.append(calc_building_score(updatedCombListDict))\n",
    "    \n",
    "    baseCombListDict = None\n",
    "    combListDict = None\n",
    "    updatedScoreDictDict = None\n",
    "    if max(updatedScoreList) > baseBuildingScore:\n",
    "        selectedScore = max([score for score in updatedScoreList if [score - baseBuildingScore]>0])\n",
    "        removeTagset = removeCandList[updatedScoreList.index(selectedScore)]\n",
    "        return (word, removeTagset)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wordUpdateIterationNum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_add_weight(word, targetTagsetType, totalWeightDict, delta=0.1):\n",
    "    tagsetTypeTotalNum = len(totalWeightDict)\n",
    "    totalWeightDict[targetTagsetType][word] += delta\n",
    "    currWeight = totalWeightDict[targetTagsetType][word]\n",
    "    if currWeight>1:\n",
    "        delta -= currWeight - 1\n",
    "        totalWeightDict[targetTagsetType][word] = 1\n",
    "    returnWeight = 0\n",
    "    for tagsetType in totalWeightDict.keys():\n",
    "        if tagsetType==targetTagsetType:\n",
    "            continue\n",
    "        totalWeightDict[tagsetType][word] -= delta / float((tagsetTypeTotalNum-1))\n",
    "        if totalWeightDict[tagsetType][word]<0:\n",
    "            returnWeight += (0-totalWeightDict[tagsetType][word])\n",
    "            totalWeightDict[tagsetType][word] = 0\n",
    "    totalWeightDict[targetTagsetType][word] -= returnWeight\n",
    "    return totalWeightDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_weight_per_comb(comb, scoreDictDict):\n",
    "    delta = 0.1\n",
    "    candSet = comb.get_cand_set()\n",
    "    origTotalWeightDict = comb.get_total_weight_dict()\n",
    "    words = origTotalWeightDict.values()[0].keys()\n",
    "    foundTotalWeightDict = defaultdict(dict)\n",
    "    tagsetTypeTotalNum = len(origTotalWeightDict)\n",
    "    for word in words:\n",
    "        totalWeightDictCandDict = dict()\n",
    "        scoreDict = dict()\n",
    "        for targetTagsetType, weightDict in origTotalWeightDict.items():\n",
    "            totalWeightDict = try_add_weight(word, targetTagsetType, deepcopy(origTotalWeightDict), delta=delta)\n",
    "            score = calc_score_altogether_in_sentence(comb.set_total_weight_dict(totalWeightDict), candSet, scoreDictDict)\n",
    "            scoreDict[targetTagsetType] = score\n",
    "            totalWeightDictCandDict[targetTagsetType] = totalWeightDict\n",
    "        \n",
    "        maxScore = max(scoreDict.values())\n",
    "        maxIdx = scoreDict.values().index(maxScore)\n",
    "        maxTagsetType = scoreDict.keys()[maxIdx]\n",
    "        \n",
    "        chosenTotalWeightDict = totalWeightDictCandDict[maxTagsetType]\n",
    "        for tagsetType in totalWeightDict.keys():\n",
    "            foundTotalWeightDict[tagsetType][word] = chosenTotalWeightDict[tagsetType][word]\n",
    "    comb.set_total_weight_dict(foundTotalWeightDict)\n",
    "    comb.set_score(calc_score_altogether_in_sentence(comb, candSet, scoreDictDict))\n",
    "    return comb\n",
    "\n",
    "def eval_weight((combListDict, scoreDictDict)):\n",
    "    newCombListDict = defaultdict(list)\n",
    "    for idx, combList in combListDict.items():\n",
    "        for comb in combList:\n",
    "            newCombListDict[idx].append(eval_weight_per_comb(comb, scoreDictDict))\n",
    "    return dict(newCombListDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_weak_candidates(combListDict):\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for idx, combList in combListDict.items():\n",
    "        if len(combList)<4:\n",
    "            continue\n",
    "        scoreList = map(scoreGetter, combList)\n",
    "        minVal = min(scoreList)\n",
    "        minIdx = scoreList.index(minVal)\n",
    "        del combList[minIdx]\n",
    "        combListDict[idx] = combList\n",
    "    return combListDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Asking words' meaning to experts\n",
    "def change_word_meaning(word, meaning, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict):\n",
    "    scoreDictDict[word] = {meaning: maxScoreDict[word]}\n",
    "    return scoreDictDict\n",
    "\n",
    "def pick_weak_word_in_cluster(idxList, combListDict, scoreDictDict):\n",
    "    adder = lambda x,y:x+y\n",
    "    wordOccurrenceDict = dict()\n",
    "    wordUsageDict = dict()\n",
    "    occIdxDict = dict()\n",
    "    cnt = 0\n",
    "    wordEntropyDict = dict()\n",
    "    candSetGetter = lambda x:x.get_cand_set()\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        wordEntropyDict[word] = stats.entropy(scoreDict.values())\n",
    "        cnt += 1\n",
    "        if cnt%100==0:\n",
    "            print cnt\n",
    "        occIndices = [idx for idx, sentence in zip(idxList, [sentenceList[i] for i in idxList]) if word in sentence]\n",
    "        occIdxDict[word] = occIndices\n",
    "        wordOccurrence = len(occIndices)\n",
    "        wordOccurrenceDict[word] = wordOccurrence\n",
    "        if wordOccurrence==0 or len(scoreDict)==1:\n",
    "            continue\n",
    "        usageScore = 0\n",
    "        for idx in occIndices:\n",
    "            combList = combListDict[idx]\n",
    "            #candSetList = map(candSetGetter, combList)\n",
    "            totalScore = sum(map(scoreGetter, combList))\n",
    "            subScore = 0\n",
    "            wordCandScoreSum = sum(scoreDict.values())\n",
    "            for wordTagsetCand, tagsetScore in scoreDict.items():\n",
    "                wordTagsetCand = wordTagsetCand.split()\n",
    "                for tagCand in wordTagsetCand:\n",
    "                    for comb in combList:\n",
    "                        for cand in comb.get_cand_set().values():\n",
    "                            cand = cand.split('_')\n",
    "                            if tagCand in cand:\n",
    "                                subScore += tagsetScore/wordCandScoreSum/len(wordTagsetCand) * comb.get_score() / totalScore / len(cand)\n",
    "            usageScore += subScore\n",
    "        wordUsageDict[word] = usageScore\n",
    "\n",
    "    percentUsageDict = dict([(word, wordUsageDict[word]/wordOccurrenceDict[word]) for word in wordUsageDict.keys()])\n",
    "\n",
    "    sortedUsage = sorted(percentUsageDict.values())\n",
    "    '''print \"========\"\n",
    "    for word, usage in sorted(percentUsageDict.items(), key=itemgetter(1)):\n",
    "        if wordOccurrenceDict[word]>2 and len(word)>1:\n",
    "            print word, usage, wordOccurrenceDict[word], wordEntropyDict[word]\n",
    "    print \"========\"\n",
    "    '''\n",
    "    return percentUsageDict\n",
    "\n",
    "#percentUsageDict = pick_weak_word_in_cluster(range(0,coverageNum), combListDict, scoreDictDict)\n",
    "\n",
    "\n",
    "def ask_experts(bestCandScoreDict, combListDict, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict):\n",
    "    answerCnt = 0\n",
    "    \n",
    "    percentUsageDict = pick_weak_word_in_cluster(range(0,coverageNum), combListDict, scoreDictDict)\n",
    "    \n",
    "    pointIdxDict = defaultdict(list)\n",
    "    for i, (bestComb, bestCand, bestScore) in bestCandScoreDict.items():\n",
    "        pointIdxDict[bestCand['point']].append(i)\n",
    "    avgScorePerPointDict = dict()\n",
    "    for key, idxList in pointIdxDict.items():\n",
    "        avgScorePerPointDict[key] = np.mean([cand[2] for cand in [bestCandScoreDict[i] for i in idxList]])\n",
    "    for key, mean in sorted(avgScorePerPointDict.items(), key=itemgetter(1)):\n",
    "        #print key, mean, len(pointIdxDict[key])\n",
    "        pass\n",
    "\n",
    "    lenGetter = lambda x: len(x[1])\n",
    "    totalScoreGetter = lambda (key,avg):avg*len(pointIdxDict[key])\n",
    "\n",
    "    weakestPointList = list()\n",
    "    #for key, idxList in sorted(pointIdxDict.items(), key=lenGetter, reverse=True):\n",
    "    #    print key, len(pointIdxDict[key])\n",
    "    for key, avg in sorted(avgScorePerPointDict.items(), key=totalScoreGetter):\n",
    "        num = len(pointIdxDict[key])\n",
    "        if num>3:\n",
    "            weakestPointList.append(key)\n",
    "            \n",
    "    #print \"=================score baesd asking\"\n",
    "    #for pointTagset in weakestPointList[0:15]:\n",
    "    #    print pick_weak_word_in_cluster(pointIdxDict[pointTagset], candListDict, scoreDictDict)\n",
    "    #print \"\\n\"\n",
    "        \n",
    "        \n",
    "    commonWordListDict = defaultdict(set)\n",
    "    adder = lambda x,y:x+y\n",
    "    for pointTagset, idxList in pointIdxDict.items():        \n",
    "        subSentenceList = [sentenceList[i] for i in idxList]\n",
    "        allWordList = list(set(reduce(adder, subSentenceList,[])))\n",
    "        for word in allWordList:\n",
    "            cnt = 0\n",
    "            for sentence in subSentenceList:\n",
    "                if word in sentence:\n",
    "                    cnt += 1\n",
    "                if cnt/float(len(idxList)) > 0.25:\n",
    "                    commonWordListDict[pointTagset].add(word)\n",
    "    \n",
    "    avgScoreGetter = lambda item: avgScorePerPointDict[item[0]]\n",
    "    commonWordListDict = OrderedDict(sorted(commonWordListDict.items(), key=avgScoreGetter))\n",
    "    \n",
    "    shownWordList = list()\n",
    "    anchorListDict = defaultdict(list)\n",
    "    for pointTagset, commonWordList in commonWordListDict.items():\n",
    "        if pointIdxDict[pointTagset]<5:\n",
    "            continue\n",
    "        for word in commonWordList:\n",
    "            if word not in shownWordList:\n",
    "                if word in percentUsageDict.keys() and len(word)>1:\n",
    "                    print word, percentUsageDict[word]\n",
    "                    shownWordList.append(word)\n",
    "                    anchorListDict[pointTagset].append((word, percentUsageDict[word]))\n",
    "    \n",
    "    itemNumGetter = lambda x:len(pointIdxDict[x[0]])\n",
    "\n",
    "    for pointTagset, anchorList in sorted(anchorListDict.items(), key=itemNumGetter, reverse=True):\n",
    "        print pointTagset\n",
    "        print '\\t', sorted(anchorList, key=itemgetter(1))\n",
    "        \n",
    "        \n",
    "        \n",
    "    resp = ''\n",
    "    while resp!='q':\n",
    "        resp = raw_input('''\n",
    "if you want to fix words, put a word to fix.\n",
    "stop asking for now and init again?\\t q\n",
    "stop asking for now but no init?\\t qq\n",
    "stop asking forever and no init?\\t qqq\n",
    "                            ''')\n",
    "        if resp=='q':\n",
    "            pass\n",
    "        elif resp=='qq':\n",
    "            return True, False, answerCnt\n",
    "        elif resp=='qqq':\n",
    "            return False, False, answerCnt\n",
    "        else:\n",
    "            word = resp\n",
    "            if word not in scoreDictDict.keys():\n",
    "                print \"What was the word again?\"\n",
    "                continue\n",
    "            meaning = raw_input(\"meaning?\\n\")\n",
    "            change_word_meaning(word, meaning, scoreDictDict)\n",
    "            answerCnt += 1\n",
    "    return True, True, answerCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_best_cand_dict(combListDict):\n",
    "    bestCandScoreDict = dict()\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for i, combList in combListDict.items():\n",
    "        scoreList = map(scoreGetter, combList)\n",
    "        maxVal = max(scoreList)\n",
    "        maxIdx = scoreList.index(maxVal)\n",
    "        bestComb = combList[maxIdx]\n",
    "        bestCandScoreDict[i] = (bestComb, bestComb.get_cand_set(), maxVal)\n",
    "    return bestCandScoreDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordUpdateIterationNum = 0\n",
    "combListDict = deepcopy(origCombListDict)\n",
    "scoreDictDict = deepcopy(origScoreDictDict)\n",
    "answerCnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "INITED\n",
      "comb init time: 0.532266333333 minutes\n",
      "weight computing time: 0.413969816667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.3114215 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733]\n",
      "================\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "vnd 0.0586128384793\n",
      "ctrl 0.00457253784281\n",
      "pid 0.0230784673006\n",
      "pct 0.0361972756733\n",
      "act 0.0421946501948\n",
      "hwp 0.0535106510141\n",
      "chwp 0.0662888745012\n",
      "actual 0.00829723929482\n",
      "ai 0.125700647298\n",
      "rf 0.0649783096761\n",
      "ah 0.0804177960338\n",
      "sp 0.179954114432\n",
      "dx 0.124536233465\n",
      "sa 0.0486084464444\n",
      "sf 0.107715568733\n",
      "da 0\n",
      "fltr 0.203224789512\n",
      "dp 0.0676917347162\n",
      "oa 0.086417345077\n",
      "ra 0.0852164592864\n",
      "chwr 0.14095350211\n",
      "mtwr 0.114721055045\n",
      "hw 0.244864435664\n",
      "sys 0.0958566061599\n",
      "mtws 0.115851608428\n",
      "pos 0.0944089153635\n",
      "dmpr 0.172828085219\n",
      "zn 0.178453172217\n",
      "acthtgsp 0.0501708888667\n",
      "actclgsp 0.040520422238\n",
      "chw 0.148189199182\n",
      "freq 0.189010503869\n",
      "sup 0.187039592923\n",
      "room_temperature_sensor\n",
      "\t[('actclgsp', 0.040520422238007767), ('acthtgsp', 0.050170888866716376), ('zn', 0.17845317221727511)]\n",
      "supply_fan_vfd_speed_sensor\n",
      "\t[('sf', 0.10771556873301856)]\n",
      "supply_fan_air_flow_sensor\n",
      "\t[('sup', 0.18703959292316913)]\n",
      "output_frequency_sensor\n",
      "\t[('freq', 0.18901050386918261)]\n",
      "damper_position_sensor\n",
      "\t[('pos', 0.094408915363526136), ('dmpr', 0.1728280852193132)]\n",
      "return_fan_vfd_speed_sensor\n",
      "\t[('rf', 0.064978309676071974)]\n",
      "air_flow_sensor\n",
      "\t[('actual', 0.0082972392948233737), ('ai', 0.12570064729814706)]\n",
      "vfd_speed_sensor\n",
      "\t[('ctrl', 0.0045725378428067001), ('pid', 0.023078467300569758), ('pct', 0.036197275673300978), ('act', 0.042194650194779394), ('hwp', 0.053510651014113071), ('vnd', 0.058612838479292648)]\n",
      "cold_box_temperature_sensor\n",
      "\t[('da', 0)]\n",
      "chilled_water_supply_temperature_sensor\n",
      "\t[('chwp', 0.066288874501227307)]\n",
      "speed_sensor\n",
      "\t[('sa', 0.048608446444358945), ('ah', 0.080417796033823596), ('dx', 0.12453623346459176), ('sp', 0.1799541144323433)]\n",
      "chilled_water_differential_pressure_sensor\n",
      "\t[('chw', 0.14818919918162154)]\n",
      "hot_water_supply_temperature_sensor\n",
      "\t[('mtws', 0.11585160842810424)]\n",
      "hot_water_discharge_temperature_sensor\n",
      "\t[('sys', 0.095856606159914468), ('mtwr', 0.11472105504496087), ('hw', 0.24486443566356461)]\n",
      "chilled_water_temperature_differential_sensor\n",
      "\t[('chwr', 0.1409535021098505)]\n",
      "outside_air_temperature_sensor\n",
      "\t[('oa', 0.086417345077008961)]\n",
      "filter_differential_pressure_sensor\n",
      "\t[('dp', 0.067691734716202823), ('fltr', 0.20322478951194012)]\n",
      "crac_temperature_sensor\n",
      "\t[('ra', 0.085216459286407864)]\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            sa\n",
      "meaning?\n",
      "supply air\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            da\n",
      "meaning?\n",
      "discharge air\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            ctrl\n",
      "meaning?\n",
      "xxxxxxxxxxxxxxx\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            actual\n",
      "meaning?\n",
      "xxxxxxxxxxxxxx\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            actclgsp\n",
      "meaning?\n",
      "effective cooling temperature setpoint\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            q\n",
      "2\n",
      "INITED\n",
      "comb init time: 0.505977483333 minutes\n",
      "weight computing time: 0.4218164 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.257627866667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144]\n",
      "================\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "vnd 0.0696537344511\n",
      "pid 0.0234221917776\n",
      "pct 0.00765009520609\n",
      "act 0.0484776754494\n",
      "hwp 0.0551011401408\n",
      "chwp 0.0982378016724\n",
      "rf 0.070562716844\n",
      "ai 0.166926700753\n",
      "sf 0.123304011464\n",
      "ah 0.0746164137932\n",
      "fltr 0.222512996422\n",
      "dx 0.155502448554\n",
      "dp 0.0801353947663\n",
      "oa 0.0894572805904\n",
      "ra 0.114946510522\n",
      "sp 0.0332773591545\n",
      "chwr 0.153435492657\n",
      "hw 0.275394598571\n",
      "sys 0.110313766198\n",
      "mtws 0.115444220044\n",
      "mtwr 0.132562152244\n",
      "pos 0.112480843045\n",
      "dmpr 0.184346090455\n",
      "zn 0.200097502419\n",
      "acthtgsp 0.0595626659204\n",
      "chw 0.168831869628\n",
      "freq 0.211924815728\n",
      "sup 0.210509040198\n",
      "room_temperature_sensor\n",
      "\t[('acthtgsp', 0.05956266592040315), ('zn', 0.20009750241928265)]\n",
      "output_frequency_sensor\n",
      "\t[('freq', 0.21192481572814381)]\n",
      "damper_position_sensor\n",
      "\t[('pos', 0.1124808430446165), ('dmpr', 0.18434609045516304)]\n",
      "supply_air_flow_sensor\n",
      "\t[('sup', 0.21050904019773314)]\n",
      "supply_fan_vfd_speed_sensor\n",
      "\t[('sf', 0.12330401146357513)]\n",
      "return_fan_vfd_speed_sensor\n",
      "\t[('rf', 0.070562716843957624)]\n",
      "air_flow_sensor\n",
      "\t[('ai', 0.16692670075320529)]\n",
      "vfd_speed_sensor\n",
      "\t[('pct', 0.0076500952060858272), ('pid', 0.023422191777612161), ('act', 0.048477675449441669), ('hwp', 0.055101140140796652), ('vnd', 0.069653734451105814)]\n",
      "chilled_water_supply_temperature_sensor\n",
      "\t[('chwp', 0.098237801672431124)]\n",
      "medium_temperature_hot_water_differential_pressure_sensor\n",
      "\t[('mtwr', 0.13256215224383144)]\n",
      "filter_differential_pressure_sensor\n",
      "\t[('ah', 0.074616413793246558), ('dp', 0.080135394766250964), ('dx', 0.15550244855440823), ('fltr', 0.22251299642183889)]\n",
      "supply_air_temperature_sensor\n",
      "\t[('sp', 0.033277359154462678)]\n",
      "chilled_water_differential_pressure_sensor\n",
      "\t[('chw', 0.16883186962772645)]\n",
      "hot_water_supply_temperature_sensor\n",
      "\t[('sys', 0.11031376619777368), ('mtws', 0.11544422004362712), ('hw', 0.27539459857142429)]\n",
      "outside_air_temperature_sensor\n",
      "\t[('oa', 0.089457280590369212)]\n",
      "chilled_water_temperature_differential_sensor\n",
      "\t[('chwr', 0.15343549265741818)]\n",
      "crac_temperature_sensor\n",
      "\t[('ra', 0.11494651052201156)]\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            sp\n",
      "meaning?\n",
      "setpoint\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            pct\n",
      "meaning?\n",
      "xxxxxxxxxxxxxxxxx\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            acthtgsp\n",
      "meaning?\n",
      "effective heating temperature setpoint\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            q\n",
      "3\n",
      "INITED\n",
      "comb init time: 0.484222683333 minutes\n",
      "weight computing time: 0.424704383333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.2465457 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653]\n",
      "================\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "vnd 0.0828674085926\n",
      "pid 0.0258374527694\n",
      "act 0.0265210832703\n",
      "hwp 0.0593914135894\n",
      "chwp 0.0815969700875\n",
      "rf 0.0761488561126\n",
      "ai 0.221495645382\n",
      "sf 0.125140960462\n",
      "ah 0.0868376415445\n",
      "fltr 0.250353474514\n",
      "dx 0.201263105365\n",
      "dp 0.103282290374\n",
      "oa 0.086907184682\n",
      "ra 0.17694837113\n",
      "chwr 0.168946048975\n",
      "hw 0.312141333876\n",
      "sys 0.125018778333\n",
      "mtws 0.115648493688\n",
      "pos 0.127257429359\n",
      "dmpr 0.196012294878\n",
      "zn 0.232499314737\n",
      "mtwr 0.149957910857\n",
      "chw 0.188085850047\n",
      "freq 0.239255402066\n",
      "room_temperature_sensor\n",
      "\t[('zn', 0.23249931473743146)]\n",
      "output_frequency_sensor\n",
      "\t[('freq', 0.23925540206554383)]\n",
      "damper_position_sensor\n",
      "\t[('pos', 0.12725742935854004), ('dmpr', 0.19601229487757801)]\n",
      "supply_fan_vfd_speed_sensor\n",
      "\t[('sf', 0.12514096046231979)]\n",
      "return_fan_vfd_speed_sensor\n",
      "\t[('rf', 0.076148856112583999)]\n",
      "air_flow_sensor\n",
      "\t[('ai', 0.2214956453816127)]\n",
      "vfd_speed_sensor\n",
      "\t[('pid', 0.025837452769390219), ('act', 0.026521083270271223), ('hwp', 0.059391413589388507), ('vnd', 0.082867408592591035)]\n",
      "medium_temperature_hot_water_differential_pressure_sensor\n",
      "\t[('mtwr', 0.14995791085674146)]\n",
      "chilled_water_supply_temperature_sensor\n",
      "\t[('chwp', 0.081596970087514875)]\n",
      "filter_differential_pressure_sensor\n",
      "\t[('ah', 0.086837641544531483), ('dp', 0.10328229037389398), ('dx', 0.20126310536480918), ('fltr', 0.25035347451406004)]\n",
      "chilled_water_differential_pressure_sensor\n",
      "\t[('chw', 0.18808585004739367)]\n",
      "hot_water_supply_temperature_sensor\n",
      "\t[('mtws', 0.11564849368815368), ('sys', 0.1250187783327171), ('hw', 0.31214133387641296)]\n",
      "outside_air_temperature_sensor\n",
      "\t[('oa', 0.086907184681985966)]\n",
      "chilled_water_temperature_differential_sensor\n",
      "\t[('chwr', 0.16894604897545074)]\n",
      "crac_temperature_sensor\n",
      "\t[('ra', 0.17694837112967798)]\n",
      "\n",
      "if you want to fix words, put a word to fix.\n",
      "stop asking for now and init again?\t q\n",
      "stop asking for now but no init?\t qq\n",
      "stop asking forever and no init?\t qqq\n",
      "                            qqq\n",
      "4\n",
      "weight computing time: 0.423752716667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.239658 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229]\n",
      "================\n",
      "5\n",
      "weight computing time: 0.405227133333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.235706883333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337]\n",
      "================\n",
      "6\n",
      "weight computing time: 0.384688416667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.254033933333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539]\n",
      "================\n",
      "7\n",
      "weight computing time: 0.3685513 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.237349983333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125]\n",
      "================\n",
      "8\n",
      "weight computing time: 0.35104175 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.252130066667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139]\n",
      "================\n",
      "9\n",
      "weight computing time: 0.334719883333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.236540583333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509]\n",
      "================\n",
      "10\n",
      "weight computing time: 0.318966333333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.234314783333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309]\n",
      "================\n",
      "11\n",
      "weight computing time: 0.302775566667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.23830015 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251]\n",
      "================\n",
      "12\n",
      "weight computing time: 0.285457283333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.228968183333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637]\n",
      "================\n",
      "13\n",
      "weight computing time: 0.270551883333 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.2201312 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034]\n",
      "================\n",
      "14\n",
      "weight computing time: 0.2509818 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.218535116667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769]\n",
      "================\n",
      "15\n",
      "weight computing time: 0.237944016667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.207035866667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533]\n",
      "================\n",
      "16\n",
      "weight computing time: 0.222414316667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.196925866667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533, 50.667459110737575]\n",
      "================\n",
      "17\n",
      "weight computing time: 0.206371466667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.192297916667 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533, 50.667459110737575, 51.775332438338161]\n",
      "================\n",
      "18\n",
      "weight computing time: 0.190722816667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.18515825 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533, 50.667459110737575, 51.775332438338161, 52.754093888345402]\n",
      "================\n",
      "19\n",
      "weight computing time: 0.17892385 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.176465883333 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533, 50.667459110737575, 51.775332438338161, 52.754093888345402, 53.774274061046142]\n",
      "================\n",
      "20\n",
      "weight computing time: 0.166895666667 (min)\n",
      "init word update\n",
      "word meaning computing time: 0.17384355 (min)\n",
      "[20.817978713623763, 22.441158871319239, 24.49959523108954, 26.81986727817862, 29.056619866048681, 31.268606432011119, 33.664954802339523, 36.452221367111903, 39.179742754149729, 41.429972669846343, 43.313536156064686, 44.454991113997323, 45.561987024467292, 46.671131381062224, 47.815444751890773, 48.8284953244194, 49.875284946016173, 50.920760402028435, 51.928444640640507, 52.819337112589501, 53.601669918708815, 54.222785237174115, 54.706639696162334, 55.019250830506124, 55.388206427736066, 55.826450795501174, 22.441158871319239, 23.758704501246733, 27.230572841347144, 29.016352488246653, 30.931075985885229, 32.983664037693337, 35.39630646630539, 37.728503767258125, 39.475543096736139, 40.997740301768509, 42.57185977775309, 44.321488824588251, 46.077644884653637, 47.22185466896034, 48.370535677805769, 49.537655452806533, 50.667459110737575, 51.775332438338161, 52.754093888345402, 53.774274061046142, 54.739529837045886]\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "iterNum = 20\n",
    "\n",
    "askFlag = True\n",
    "initFlag = True\n",
    "\n",
    "reduceCandNum = 0\n",
    "for iterCnt in range(0,iterNum):\n",
    "    print wordUpdateIterationNum\n",
    "    \n",
    "    if initFlag:\n",
    "        print \"INITED\"\n",
    "        ######################### Init Cand \n",
    "        beginTime =  datetime.now()\n",
    "        if mpFlag:\n",
    "            pool = multiprocessing.Pool(workerNum)\n",
    "            combListList = pool.map(init_comb, zip(sentenceList, repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "            pool.terminate()\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            combListList = map(init_comb, zip(sentenceList, repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "            pass\n",
    "        combListDict = dict((i,combList) for i, combList in enumerate(combListList))\n",
    "        endTime = datetime.now()\n",
    "        print \"comb init time:\", (endTime-beginTime).total_seconds()/60, 'minutes'\n",
    "    \n",
    "    \n",
    "    #print iterCnt\n",
    "    ########################## Update Weights\n",
    "    beginTime =  datetime.now()\n",
    "    makeDict = lambda x:dict([x])\n",
    "    adder = lambda x,y:x+y\n",
    "    dictgetter = lambda x:x.items()\n",
    "    if mpFlag:\n",
    "        pool = multiprocessing.Pool(workerNum)\n",
    "        updatedDictList = pool.map(eval_weight, zip(map(makeDict,combListDict.items()), repeat(scoreDictDict)))\n",
    "        pool.terminate()\n",
    "        #pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        updatedDictList = map(eval_weight, zip(map(makeDict,combListDict.items()), repeat(scoreDictDict)))\n",
    "    combListDict = dict(reduce(adder,map(dictgetter, updatedDictList)))\n",
    "    \n",
    "    endTime = datetime.now()\n",
    "    print \"weight computing time:\", (endTime-beginTime).total_seconds()/60, '(min)'\n",
    "    \n",
    "\n",
    "    #################### Update Word Meaning\n",
    "    print \"init word update\"\n",
    "    \n",
    "    baseBuildingScore = calc_building_score(combListDict)\n",
    "    \n",
    "    beginTime =  datetime.now()\n",
    "    if mpFlag:\n",
    "        pool = multiprocessing.Pool(workerNum)\n",
    "        removeWordTagsetTupleList = pool.map(eval_word_meaning2, zip(repeat(combListDict), scoreDictDict.keys(), \\\n",
    "                                                                     repeat(scoreDictDict)))\n",
    "        pool.terminate()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        removeWordTagsetTupleList = map(eval_word_meaning2, zip(repeat(combListDict), scoreDictDict.keys(), repeat(scoreDictDict)))\n",
    "    removeWordTagsetDict = dict([el for el in removeWordTagsetTupleList if el])\n",
    "    endTime = datetime.now()\n",
    "\n",
    "    #update removing word meanings\n",
    "    for word, tagset in removeWordTagsetDict.items():\n",
    "        del scoreDictDict[word][tagset]\n",
    "        \n",
    "    combListDict = update_comb_cand_dict_score(combListDict, scoreDictDict)\n",
    "    buildingScoreList.append(calc_building_score(combListDict))\n",
    "    wordUpdateIterationNum += 1\n",
    "    \n",
    "\n",
    "    ################## Remove weak candidates\n",
    "    combListDict = remove_weak_candidates(combListDict)\n",
    "    print \"word meaning computing time:\", (endTime-beginTime).total_seconds()/60, '(min)'\n",
    "    print buildingScoreList\n",
    "    print \"================\"\n",
    "    \n",
    "    ########## Calculate best candidates\n",
    "    bestCandScoreDict = calc_best_cand_dict(combListDict)\n",
    "    #scoreGetter = lambda x:x.get_score()\n",
    "    #for i, combList in combListDict.items():\n",
    "    #    scoreList = map(scoreGetter, combList)\n",
    "    #    maxVal = max(scoreList)\n",
    "    #    maxIdx = scoreList.index(maxVal)\n",
    "    #    bestComb = combList[maxIdx]\n",
    "    #    bestCandScoreDict[i] = (bestComb, bestComb.get_cand_set(), maxVal)\n",
    "        \n",
    "        \n",
    "    if askFlag:\n",
    "        askFlag, initFlag, answerNum = ask_experts(bestCandScoreDict, combListDict, scoreDictDict)\n",
    "        answerCnt += answerNum\n",
    "        \n",
    "print \"Human Inputs: \", answerCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 {'equip': u'ahu', 'location': u'cold_box', 'point': u'return_fan_air_flow_sensor'} 0.425515965285\n",
      "7 {'equip': u'ahu', 'location': u'environment_box', 'point': u'return_fan_air_flow_sensor'} 0.401983934937\n",
      "10 {'equip': u'ahu', 'location': u'hot_box', 'point': u'return_fan_air_flow_sensor'} 0.401983934937\n",
      "1 {'equip': u'ahu', 'point': u'return_fan_air_flow_sensor'} 0.39681165267\n",
      "4 {'equip': u'ahu', 'location': u'cold_box', 'point': u'return_air_flow_sensor'} 0.373588114595\n",
      "5 {'equip': u'ahu', 'location': u'cold_box', 'point': u'discharge_fan_air_flow_sensor'} 0.361119849677\n",
      "8 {'equip': u'ahu', 'location': u'environment_box', 'point': u'return_air_flow_sensor'} 0.350056084246\n",
      "11 {'equip': u'ahu', 'location': u'hot_box', 'point': u'return_air_flow_sensor'} 0.350056084246\n",
      "2 {'equip': u'ahu', 'point': u'return_air_flow_sensor'} 0.341573373262\n",
      "9 {'equip': u'ahu', 'location': u'environment_box', 'point': u'discharge_fan_air_flow_sensor'} 0.337587819329\n",
      "12 {'equip': u'ahu', 'location': u'hot_box', 'point': u'discharge_fan_air_flow_sensor'} 0.337587819329\n",
      "13 {'location': u'cold_box', 'point': u'return_fan_air_flow_sensor'} 0.328400512703\n",
      "16 {'location': u'environment_box', 'point': u'return_fan_air_flow_sensor'} 0.304648727013\n",
      "17 {'location': u'hot_box', 'point': u'return_fan_air_flow_sensor'} 0.304648727013\n",
      "0 {'point': u'return_fan_air_flow_sensor'} 0.293650854142\n",
      "14 {'location': u'cold_box', 'point': u'fan_air_flow_sensor'} 0.273362554826\n",
      "15 {'location': u'cold_box', 'point': u'return_air_flow_sensor'} 0.273362554826\n",
      "6 {'equip': u'fan', 'location': u'cold_box', 'point': u'fan_air_flow_sensor'} 0.264921616918\n"
     ]
    }
   ],
   "source": [
    "candGetter = lambda x:x.get_cand_set()\n",
    "scoreGetter = lambda x:x[1].get_score()\n",
    "for (i,comb) in sorted(enumerate(combListDict[148]), key=scoreGetter, reverse=True):\n",
    "    print i, candGetter(comb), comb.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<__main__.SentenceTagsetsCombination instance at 0x7f8a80556fc8>,\n",
       " {'equip': u'ahu', 'point': u'return_fan_air_flow_sensor'},\n",
       " 0.40470387882709136)"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestCandScoreDict[148]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_correct(predLabel, currLabel):\n",
    "    currLabel = currLabel.replace(' ', '_')\n",
    "    possibleLabelList = [currLabel]\n",
    "    if 'supply' in currLabel:\n",
    "        possibleLabelList.append(currLabel.replace('supply', 'discharge'))\n",
    "    if 'discharge' in currLabel:\n",
    "        possibleLabelList.append(currLabel.replace('discharge', 'supply'))\n",
    "    if currLabel in equalDict.keys():\n",
    "        possibleLabelList.append(equalDict[currLabel])\n",
    "    if predLabel in possibleLabelList:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = 'result/'+buildingName+'_comb_cand.csv'\n",
    "with open(filename, 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(['sentence', 'true point type', 'true equip type', 'point correct?', 'candidate combi', 'score', 'candidate combi', 'score'])\n",
    "    addFunc = lambda x,y:x+y\n",
    "\n",
    "    candScoreGetter = lambda x:(x.get_cand_set(), x.get_score())\n",
    "    \n",
    "    for i, combList in combListDict.items():\n",
    "        #candTupleList = reduce(addFunc,map(itemgetter(1), tagsetTypeBasedList),[])\n",
    "        #candTupleList = sorted(candTupleList, key=itemgetter(1), reverse=True)\n",
    "        candTupleList = map(candScoreGetter, combList)\n",
    "        candTupleList = sorted(candTupleList, key=itemgetter(1), reverse=True)\n",
    "        comb = combList[0]\n",
    "        if not buildingName in notUcsdBuildings:\n",
    "            srcid = sensorDF.index[i]\n",
    "            trueRow = trueDF.loc[srcid]\n",
    "            pointCorrect = check_correct(bestCandScoreDict[i][1]['point'], trueRow['Schema Label'])\n",
    "            writeList = [comb.get_sentence(), trueRow['Schema Label'], trueRow['Equipment Type'], 1 if pointCorrect else 0]\n",
    "        else:\n",
    "            label = df.iloc[i]['Schema Label']\n",
    "            writeList = [comb.get_sentence(), label]\n",
    "        \n",
    "        for candTuple in candTupleList:\n",
    "            writeList.append(candTuple[0])\n",
    "            writeList.append(candTuple[1])\n",
    "        writer.writerow(writeList)\n",
    "f = gdrive.CreateFile()\n",
    "f.SetContentFile(filename)\n",
    "f.Upload()\n",
    "f = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transfer knowledge\n",
    "\n",
    "foundWordDict = OrderedDict()\n",
    "cnt = 0\n",
    "for (comb, candSet, candSetScore) in sorted(bestCandScoreDict.values(), key=itemgetter(2), reverse=True):\n",
    "    if cnt>coverageNum*0.2:\n",
    "        break\n",
    "    cnt +=1\n",
    "    sentence = comb.get_sentence()\n",
    "    for word in sentence:\n",
    "        meaningTagsetList = scoreDictDict[word].keys()\n",
    "        if len(meaningTagsetList)>1:\n",
    "            continue\n",
    "        meaningTagset = meaningTagsetList[0]\n",
    "        correctFlag = True\n",
    "        for meaningTag in meaningTagset.split():\n",
    "            pointTagset = candSet['point'].split('_')\n",
    "            if 'equip' in candSet.keys():\n",
    "                equipTagset = candSet['equip'].split('_')\n",
    "            else:\n",
    "                equipTagset = []\n",
    "            if meaningTag not in pointTagset+equipTagset:\n",
    "                correctFlag = False\n",
    "        if correctFlag:\n",
    "            foundWordDict[word] = meaningTagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!!!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"etc/fins_success.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"Finished!!!\"\n",
    "from IPython.display import Audio\n",
    "sound_file = 'etc/fins_success.wav'\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combCandDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-391-f396303cfd89>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlabelSentenceList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mselectedTagsetDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandComb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcombCandDict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mdummyWeightDict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbestCandScoreDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weight_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dummy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mlabelWordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'combCandDict' is not defined"
     ]
    }
   ],
   "source": [
    "# select best cand set.\n",
    "labelSentenceList = list()\n",
    "selectedTagsetDict = dict()\n",
    "for i, candComb in combCandDict.items():\n",
    "    dummyWeightDict = bestCandScoreDict[i][0].get_weight_dict('dummy')\n",
    "    labelWordList = list()\n",
    "    for word, weight in dummyWeightDict.items():\n",
    "        if weight>0.8:\n",
    "            labelWordList.append(word)\n",
    "    for word in rawSentenceList[i]:\n",
    "        if word.isdigit():\n",
    "            labelWordList.append(word)\n",
    "    labelSentenceList.append(labelWordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word = 'ah'\n",
    "print scoreDictDict[word]\n",
    "print origScoreDictDict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combList[6].get_total_weight_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combList = origCombListDict[143]\n",
    "for comb in combList:\n",
    "    print comb.get_cand_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(combCandDict)):\n",
    "    if 'act' in combCandDict[i][0][0].get_sentence():\n",
    "        print i, combCandDict[i][0][0].get_sentence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
