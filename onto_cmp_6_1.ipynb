{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visit here: https://accounts.google.com/o/oauth2/auth?scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.file+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.install&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&client_id=842684053512-91e0897dkl1iuiuunb0mkfkdqddi41fl.apps.googleusercontent.com&access_type=offline\n",
      "4/MHV2LvAH6Ltt4XDCavBF-FGwtKycyUC8T_Dm6Ky-K98\n",
      "Authentication successful.\n",
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n",
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n"
     ]
    }
   ],
   "source": [
    "#essential libraries\n",
    "\n",
    "from google_drive import gdrive\n",
    "\n",
    "from matplotlib.pyplot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "import matplotlib\n",
    "reload(matplotlib)\n",
    "#matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter, defaultdict, OrderedDict, deque\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy, copy\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "import os\n",
    "import gc\n",
    "import linecache\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "from itertools import repeat\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import URIRef\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy import spatial\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy import stats\n",
    "from numpy.testing import assert_almost_equal\n",
    "\n",
    "from scipy.spatial.distance import cosine as cosine_similarity\n",
    "\n",
    "from divergence import gau_js as js_divergence\n",
    "import building_tokenizer as toker\n",
    "import brick_parser\n",
    "reload(brick_parser)\n",
    "from brick_parser import tagList, tagsetList, equipTagsetList, pointTagsetList, locationTagsetList,\\\n",
    "equalDict, pointTagList, equipTagList, locationTagList, equipPointDict\n",
    "subTagListDict = dict([('point', pointTagList),\n",
    "                          ('equip', equipTagList),\n",
    "                          ('location', locationTagList)\n",
    "                         ])\n",
    "subTagsetListDict = dict([('point', pointTagsetList),\n",
    "                          ('equip', equipTagsetList),\n",
    "                          ('location', locationTagsetList)\n",
    "                         ])\n",
    "\n",
    "#from cmu_parser import cmu_building_parse\n",
    "\n",
    "debugFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "workerNum = 7\n",
    "coverageNum = 4100\n",
    "mpFlag = True\n",
    "\n",
    "coverageNumDict = {'bonner':3213,\n",
    "                   'ap_m':4380,\n",
    "                   'ebu3b':4593\n",
    "                  }\n",
    "\n",
    "onlySampleFlag = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_fig(fig, name, dpi=400):\n",
    "\tpp = PdfPages(name)\n",
    "\tpp.savefig(fig, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\tpp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_dict_with_best_n_from_dict(srcDict, n):\n",
    "    idxCnt = 0\n",
    "    sortedValueList = sorted(srcDict.values(), reverse=True)[0:n]\n",
    "    chosenItemList = list()\n",
    "    \n",
    "    for score in sortedValueList:\n",
    "        for key, val in srcDict.items():\n",
    "            if idxCnt>=n:\n",
    "                break\n",
    "            if val==score:\n",
    "                chosenItemList.append((key,val))\n",
    "                idxCnt += 1\n",
    "        if idxCnt>=n:\n",
    "                break\n",
    "    return OrderedDict(chosenItemList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from sys import getsizeof, stderr\n",
    "from itertools import chain\n",
    "from collections import deque\n",
    "try:\n",
    "    from reprlib import repr\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "def total_size(o, handlers={}, verbose=False):\n",
    "    \"\"\" Returns the approximate memory footprint an object and all of its contents.\n",
    "\n",
    "    Automatically finds the contents of the following builtin containers and\n",
    "    their subclasses:  tuple, list, deque, dict, set and frozenset.\n",
    "    To search other containers, add handlers to iterate over their contents:\n",
    "\n",
    "        handlers = {SomeContainerClass: iter,\n",
    "                    OtherContainerClass: OtherContainerClass.get_elements}\n",
    "\n",
    "    \"\"\"\n",
    "    dict_handler = lambda d: chain.from_iterable(d.items())\n",
    "    all_handlers = {tuple: iter,\n",
    "                    list: iter,\n",
    "                    deque: iter,\n",
    "                    dict: dict_handler,\n",
    "                    set: iter,\n",
    "                    frozenset: iter,\n",
    "                   }\n",
    "    all_handlers.update(handlers)     # user handlers take precedence\n",
    "    seen = set()                      # track which object id's have already been seen\n",
    "    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n",
    "\n",
    "    def sizeof(o):\n",
    "        if id(o) in seen:       # do not double count the same object\n",
    "            return 0\n",
    "        seen.add(id(o))\n",
    "        s = getsizeof(o, default_size)\n",
    "\n",
    "        if verbose:\n",
    "            print(s, type(o), repr(o))#, file=stderr)\n",
    "\n",
    "        for typ, handler in all_handlers.items():\n",
    "            if isinstance(o, typ):\n",
    "                s += sum(map(sizeof, handler(o)))\n",
    "                break\n",
    "        return s\n",
    "\n",
    "    return sizeof(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_equip_points(equip, g):\n",
    "    pointList = list()\n",
    "    hasPointRef = URIRef(uriPrefix+'hasPoint')\n",
    "    if equip=='vav':\n",
    "        equipRef = URIRef(uriPrefix+'VAV')\n",
    "    elif equip=='ahu':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='vfd':\n",
    "        equipRef = URIRef(uriPrefix+'VFD')\n",
    "    elif equip=='chilled_water_pump':\n",
    "        equipRef = URIRef(uriPrefix+'Chilled_Water_Pump')\n",
    "    elif equip=='hot_water_pump':\n",
    "        equipRef = URIRef(uriPrefix+'Hot_Water_Pump')\n",
    "    elif equip=='crac':\n",
    "        equipRef = URIRef(uriPrefix+'CRAC')\n",
    "    elif equip=='exhaust_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='return_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    elif equip=='supply_fan':\n",
    "        equipRef = URIRef(uriPrefix+'AHU')\n",
    "    #elif equip=='return_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Return_Fan')\n",
    "    #elif equip=='exhaust_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Exhaust_Fan')\n",
    "    #elif equip=='supply_fan':\n",
    "    #    equipRef = URIRef(uriPrefix+'Supply_Fan')\n",
    "    else:\n",
    "        print (\"equip name %s is not in the schema\" % equip)\n",
    "        assert(False)\n",
    "    for superClass in g.objects(equipRef,RDFS.subClassOf):\n",
    "        #print triple\n",
    "        #for tri in g.triples((triple, None, None)):\n",
    "        if (superClass, OWL.onProperty, hasPointRef) in g:\n",
    "            for point in g.objects(superClass, OWL.someValuesFrom):\n",
    "                pointList.append(point.decode().split(\"#\")[1].lower().split('_'))\n",
    "    return pointList\n",
    "\n",
    "def equip_ref_to_type(equipRef):\n",
    "    if equipRef==None:\n",
    "        return None\n",
    "    equipRefList = equipRef.split('_')\n",
    "    equipTypeList = list()\n",
    "    for word in equipRefList:\n",
    "        numList = re.findall('\\d+', word)\n",
    "        if len(numList)==0:\n",
    "            equipTypeList.append(word)\n",
    "    return str('_'.join(equipTypeList))\n",
    "\n",
    "def equip_ref_to_ref(equipRef):\n",
    "    if equipRef==None:\n",
    "        return None\n",
    "    equipRefList = equipRef.split('_')\n",
    "    equipTypeList = list()\n",
    "    for word in equipRefList:\n",
    "        numList = re.findall('\\d+', word)\n",
    "        if len(numList)!=0:\n",
    "            return word\n",
    "    return '9999'\n",
    "\n",
    "def get_alpha_word(word):\n",
    "    alphaWordList = re.findall('[a-zA-Z]+', word)\n",
    "    if len(alphaWordList)>0:\n",
    "        alphaWord = alphaWordList[0]\n",
    "        return alphaWord\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_num_word(word):\n",
    "    wordList = re.findall('[b]?\\d+[abc]?\\_?\\d*', word)\n",
    "    if len(wordList)>0:\n",
    "        foundWord = wordList[0]\n",
    "        return foundWord\n",
    "    else:\n",
    "        return '9999'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SentenceTagsetsCombination:\n",
    "    totalWeightDict = None\n",
    "    sentence = None\n",
    "    \n",
    "    def determine_word_belongs_to_sentence(self, word, tagList, scoreDictDict):\n",
    "        #closestTagList = scoreDictDict[word].keys()[0].split()\n",
    "        #return sum([1.0 if tag in tagList else 0.0 for tag in closestTagList]) / float(len(closestTagList)) >=0.2\n",
    "        subScoreDict = pick_dict_with_best_n_from_dict(scoreDictDict[word], 3)\n",
    "        totalScore = 0\n",
    "        for tagset, score in subScoreDict.items():\n",
    "            for tag in tagset.split():\n",
    "                if tag in tagList:\n",
    "                    totalScore += score / float(len(tagset.split()))\n",
    "        #if word=='meter':\n",
    "        #    print totalScore\n",
    "        totalScore /= sum(subScoreDict.values())\n",
    "        #return score>=0.2\n",
    "        \n",
    "        return totalScore>=0.2\n",
    "    \n",
    "    def __init__(self, sentence, subTagListDict, scoreDictDict, candSet=None):\n",
    "        self.candSet = candSet\n",
    "        self.scoreHistory = [0]\n",
    "        self.sentence = sentence\n",
    "        wordScoreDictDict = dict()\n",
    "        for tagsetType in subTagListDict.keys()+['dummy']:\n",
    "            wordScoreDictDict[tagsetType] = dict()\n",
    "        \n",
    "        for word in sentence:\n",
    "            if len(word)<=1 or '' in scoreDictDict[word].keys():\n",
    "                wordScoreDictDict['dummy'][word] = 1.0\n",
    "                for tagsetType in subTagListDict.keys():\n",
    "                    wordScoreDictDict[tagsetType][word] = 0.0\n",
    "                continue\n",
    "            \n",
    "            wordOccurrenceDict = dict()\n",
    "            for tagsetType, tagList in subTagListDict.items():\n",
    "                wordOccurrenceDict[tagsetType] = 1.0 if self.determine_word_belongs_to_sentence(word, tagList, scoreDictDict) else 0.0\n",
    "            totalOcc = sum(wordOccurrenceDict.values())\n",
    "            \n",
    "            #if word=='meter': ##########\n",
    "            #    print wordOccurrenceDict\n",
    "            \n",
    "            if totalOcc==0:\n",
    "                wordScoreDictDict['dummy'][word] = 1.0\n",
    "                for tagsetType in wordOccurrenceDict.keys():\n",
    "                    wordScoreDictDict[tagsetType][word] = 0.0\n",
    "            else:\n",
    "                wordScoreDictDict['dummy'][word]= 0.0\n",
    "                for tagsetType, wordOccurrence in wordOccurrenceDict.items():\n",
    "                    wordScoreDictDict[tagsetType][word] = wordOccurrence / totalOcc\n",
    "        \n",
    "        self.totalWeightDict = wordScoreDictDict\n",
    "        \n",
    "    def set_cand_set(self, candSet, score):\n",
    "        selfTagsetTypes = set(self.totalWeightDict.keys())\n",
    "        selfTagsetTypes.remove('dummy')\n",
    "        \n",
    "        assert(set(candSet.keys())==selfTagsetTypes)\n",
    "        self.candSet = candSet\n",
    "        self.scoreHistory.append(score)\n",
    "        \n",
    "    def set_score(self, score):\n",
    "        self.scoreHistory.append(score)\n",
    "    \n",
    "    def get_score(self):\n",
    "        return self.scoreHistory[-1]\n",
    "    \n",
    "    def get_cand_set(self):\n",
    "        return self.candSet\n",
    "        \n",
    "    def set_sentence(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        \n",
    "    def get_sentence(self):\n",
    "        return self.sentence\n",
    "    \n",
    "    def get_weight_dict(self, tagsetType):\n",
    "        return self.totalWeightDict[tagsetType]\n",
    "    \n",
    "    def get_total_weight_dict(self):\n",
    "        return self.totalWeightDict\n",
    "    \n",
    "    def set_total_weight_dict(self, newWeightDict):\n",
    "        aWeightDict = self.totalWeightDict['dummy']\n",
    "        assert(len(newWeightDict.keys())==len(self.totalWeightDict.keys()))\n",
    "        for word in aWeightDict.keys():\n",
    "            assert(np.allclose(1.0, sum([weightDict[word] for weightDict in newWeightDict.values()])))\n",
    "        self.totalWeightDict = newWeightDict\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SubSentenceTagsetPair:\n",
    "    localScoreDict = dict() # similar to scoreDictDict but only contains words existing in the sentence\n",
    "    sentence = list()\n",
    "    targetTagset = ''\n",
    "    origSentence = list()\n",
    "    predLable = ''\n",
    "    score = 0 \n",
    "    direction = ''\n",
    "    scoreHistory = []\n",
    "    \n",
    "    def __init__(self, origSentence, localScoreDict, targetTagset, score=0):\n",
    "        self.origSentence = origSentence\n",
    "        self.sentence = origSentence\n",
    "        self.localScoreDict = localScoreDict\n",
    "        self.score = score\n",
    "        self.scoreHistory = [score]\n",
    "        self.targetTagset = targetTagset\n",
    "    \n",
    "    def append_score_history(self, score):\n",
    "        self.scoreHistory.append(score)\n",
    "    def get_score_history(self):\n",
    "        return self.scoreHistory\n",
    "    def set_score_dict(self, localScoreDict):\n",
    "        self.localScoreDict = localScoreDict\n",
    "    def set_direction(self, flag):\n",
    "        if not (flag=='+' or flag=='-'):\n",
    "            print 'wrong direction flag'\n",
    "            assert(False)\n",
    "        else:\n",
    "            self.direction = flag\n",
    "    def get_direction(self):\n",
    "        return self.direction\n",
    "    def get_score_dict(self):\n",
    "        return self.localScoreDict\n",
    "    def set_sentence(self, sentence):\n",
    "        self.sentence = sentence\n",
    "    def get_sentence(self):\n",
    "        return self.sentence\n",
    "    def set_score(self, score):\n",
    "        self.score = score\n",
    "    def get_score(self):\n",
    "        return self.score\n",
    "    def get_target_tagset(self):\n",
    "        return self.targetTagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildingName = 'ap_m'\n",
    "notUcsdBuildings = ['ghc']\n",
    "\n",
    "tokenType = 'NoNumber'\n",
    "\n",
    "if not buildingName in notUcsdBuildings:\n",
    "    naeDict = dict()\n",
    "    naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "    naeDict['ap_m'] = ['514', '513','604']\n",
    "    naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "    naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "    #naeDict['otterson'] = [\"518\", \"517\", \"589\", \"590\"]\n",
    "    naeList = naeDict[buildingName]\n",
    "\n",
    "    labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "    with open(labeledFile, 'rb') as fp:\n",
    "        #truthDF = pd.read_excel(fp)\n",
    "        truthDF = pd.DataFrame.from_csv(fp)\n",
    "        #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "    wordFeatFile = 'data/wordfeat_'+buildingName+'.pkl'\n",
    "\n",
    "    tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "    bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "    for val in Counter(unitMap.keys()).values():\n",
    "        if val>1:\n",
    "            \"Unit map file ERROR\"\n",
    "            assert(False)\n",
    "            \n",
    "            \n",
    "    trueDF = pd.DataFrame.from_csv('metadata/'+buildingName+'_sensor_types_location.csv')\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, bacnettypeList, wordList = \\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "\n",
    "    origSensorDF = deepcopy(sensorDF)\n",
    "    origNameList = deepcopy(nameList)\n",
    "    origJcinameList = deepcopy(jcinameList)\n",
    "    origDescList = deepcopy(descList)\n",
    "    origUnitList = deepcopy(unitList)\n",
    "    origBacnettypeList = deepcopy(bacnettypeList)\n",
    "    origWordList = deepcopy(wordList)\n",
    "\n",
    "    _, rawNameList, rawJcinameList, rawDescList, _, _, _ = toker.structure_metadata(buildingName=buildingName, tokenType='AlphaAndNum', \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "    \n",
    "    \n",
    "else: \n",
    "    filename = 'metadata/'+buildingName+'_sensor_types_location.csv'\n",
    "    #   filename = 'metadata/%s_sensor_types_location.csv'%buildingName\n",
    "    df = pd.read_csv(filename)\n",
    "    sentenceDict = dict()    \n",
    "    for i,raw in enumerate(df['bas_raw'].tolist()):\n",
    "        sentenceDict[i] = toker.tokenize(tokenType, raw)\n",
    "    \n",
    "    \n",
    "## Common part\n",
    "    \n",
    "with open('data/'+buildingName+'_str_score_dict.pkl', 'rb') as fp:\n",
    "    scoreDictDict = pickle.load(fp)\n",
    "    ### If a word is exactly matched with another, fix it.\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        if word in scoreDict.keys():\n",
    "            scoreDictDict[word] = {word:1}\n",
    "    scoreDictDict['co'] = {'co2':1}\n",
    "        \n",
    "adder = lambda x,y:x+y\n",
    "splitter = lambda x:x.split()\n",
    "bacnetTypes = list(set(reduce(adder,map(splitter,origBacnettypeList),[])))\n",
    "for bacnetType in bacnetTypes:\n",
    "    scoreDictDict[bacnetType] = {bacnetType:1}\n",
    "units = list(set(unitMap['word'].tolist()))\n",
    "for unit in units:\n",
    "    if type(unit)==str:\n",
    "        scoreDictDict[unit] = {unit:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[136, 1535, 2834, 3052, 1232, 805, 33, 2673, 3436, 2841, 3498, 2585, 3356, 1429, 1212, 2451, 957, 3574, 3695, 2818, 3823, 123, 3176, 475, 2061, 1107, 1677, 291, 2094, 3685, 3314, 2540, 1512, 2021, 3236, 3571, 308, 56, 313, 2800, 2576, 2163, 342, 2438, 913, 2104, 2374, 2316, 3593, 1555, 1118, 3468, 2035, 1538, 1453, 434, 2177, 2580, 278, 223, 1693, 1804, 385, 2337, 2256, 3540, 2422, 1914, 1008, 3616, 2375, 1813, 1967, 681, 1712, 881, 1792, 3153, 2678, 1394, 3415, 2270, 2262, 3293, 3976, 782, 2974, 786, 583, 3626, 3304, 2143, 2016, 1363, 3552, 60, 3430, 472, 390, 775]\n"
     ]
    }
   ],
   "source": [
    "randNum = 100\n",
    "randIdxList = random.sample(range(0,4000), randNum)\n",
    "print randIdxList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 47, 59, 82, 146, 163, 224, 263, 269, 303, 330, 390, 396, 412, 448, 525, 566, 600, 602, 613, 633, 636, 640, 652, 710, 749, 786, 866, 1019, 1038, 1055, 1090, 1098, 1119, 1143, 1226, 1338, 1750, 1762, 1765, 1777, 1799, 1813, 1823, 1824, 1857, 1927, 1970, 1994, 2069, 2078, 2089, 2100, 2113, 2175, 2208, 2242, 2273, 2276, 2373, 2435, 2442, 2515, 2533, 2553, 2564, 2593, 2604, 2693, 2706, 2719, 2784, 2826, 2847, 2862, 2867, 2960, 2967, 2983, 3114, 3177, 3226, 3228, 3301, 3325, 3333, 3336, 3352, 3474, 3551, 3554, 3565, 3574, 3578, 3587, 3619, 3647, 3746, 3778, 3786]\n"
     ]
    }
   ],
   "source": [
    "#print randIdxList\n",
    "randIdxList = sorted([224, 3226, 2175, 3474, 602, 146, 1813, 640, 1055, 3778, 2276, 1019, 390, 2208, 3301, 1090, 2089, 10, 1143, 2113, 1927, 3325, 2533, 1098, 1777, 3352, 710, 1765, 3228, 59, 2867, 2273, 3551, 269, 1119, 2373, 3647, 330, 2983, 1994, 2593, 2784, 3578, 786, 3574, 1038, 2826, 613, 3786, 2862, 2435, 2847, 2693, 2442, 3554, 1750, 163, 412, 3746, 3619, 3114, 3565, 1970, 636, 1338, 1226, 2100, 566, 2553, 303, 1857, 2960, 652, 448, 3333, 600, 47, 1762, 633, 263, 2564, 2069, 525, 396, 749, 3177, 3587, 2719, 2967, 3336, 2706, 2515, 1823, 2242, 1824, 866, 82, 2078, 1799, 2604])\n",
    "print randIdxList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4380"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sensorDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['514_0_3006443', '514_0_3006661', '514_0_3006711', '514_0_3006822', '514_0_3007073', '514_0_3007151', '514_0_3007388', '514_0_3007528', '514_0_3007553', '514_0_3011251', '514_1_3006410', '514_1_3006647', '514_1_3006658', '514_1_3006699', '514_1_3006791', '514_1_3007025', '514_1_3007140', '514_1_3007267', '514_1_3007269', '514_1_3007304', '514_1_3007372', '514_1_3007377', '514_1_3007393', '514_1_3007438', '514_1_3011258', '514_2_3010337', '514_3_3007491', '514_4_3007474', '513_0_3005109', '513_0_3005200', '513_0_3005272', '513_0_3005401', '513_0_3005420', '513_0_3005492', '513_0_3005621', '513_0_3006007', '513_0_3006493', '513_1_3004779', '513_1_3004816', '513_1_3004820', '513_1_3004848', '513_1_3004909', '513_1_3004940', '513_1_3004964', '513_1_3004968', '513_1_3005054', '513_1_3005234', '513_1_3005346', '513_1_3005524', '513_1_3005727', '513_1_3005750', '513_1_3005775', '513_1_3005811', '513_1_3005841', '513_1_3006004', '513_1_3006091', '513_1_3006180', '513_1_3006256', '513_1_3006262', '513_1_3006544', '513_1_3006749', '513_1_3006772', '513_1_3007022', '513_1_3007085', '513_1_3007153', '513_1_3007189', '513_1_3007279', '513_1_3007318', '513_1_3011820', '513_1_3015242', '513_2_3003433', '513_3_3004511', '513_4_3004520', '513_4_3004983', '513_4_3005328', '513_4_3005384', '513_14_3006606', '513_14_3006684', '513_14_3006852', '604_0_3012569', '604_0_3012715', '604_0_3012834', '604_0_3012839', '604_0_3013050', '604_0_3013124', '604_0_3013144', '604_0_3013156', '604_0_3013203', '604_0_3013496', '604_0_3013661', '604_0_3013670', '604_0_3014280', '604_1_3012397', '604_1_3012407', '604_1_3012422', '604_1_3012471', '604_1_3012518', '604_1_3012757', '604_1_3012831', '604_1_3012853']\n"
     ]
    }
   ],
   "source": [
    "randSrcidList = list()\n",
    "for randIdx in randIdxList:\n",
    "    if randIdx <len(sensorDF):\n",
    "        srcid = sensorDF.iloc[randIdx].name\n",
    "    randSrcidList.append(srcid)\n",
    "print randSrcidList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if onlySampleFlag:\n",
    "    idxList = randIdxList\n",
    "else:\n",
    "    idxList = range(0,len(trueDF))\n",
    "\n",
    "\n",
    "listIndexFunc = lambda l, i: [l[ii] for ii in i]\n",
    "#sensorDF = origSensorDF.iloc[idxList]\n",
    "sensorDF = origSensorDF\n",
    "nameList = listIndexFunc(origNameList, idxList)\n",
    "jcinameList = listIndexFunc(origJcinameList, idxList)\n",
    "descList = listIndexFunc(origDescList, idxList)\n",
    "unitList = listIndexFunc(origUnitList, idxList)\n",
    "bacnettypeList = listIndexFunc(origBacnettypeList, idxList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_max_score_dict(scoreDictDict):\n",
    "    maxScoreDict = dict()\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        maxScoreDict[word] = max(scoreDict.values())\n",
    "    return maxScoreDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def manual_def(word, trueTagset):\n",
    "    scoreDict = scoreDictDict[word]\n",
    "    scoreDictDict[word] = dict((tagset,1) if turTagset==tagset else (tagset,0) for tagset in scoreDict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Initialization sentence separation\n",
    "#### We need equip sentence\n",
    "### We need type sentence, equip sentence, location sentence\n",
    "\n",
    "# Init no meaning sentence\n",
    "sentenceDict = list()\n",
    "adder = lambda a,b:a+b\n",
    "splitter = lambda s:s.split()\n",
    "sentenceDict = dict([(i,reduce(adder, map(splitter, dataList)))\\\n",
    "                for i,dataList in zip(idxList, zip(nameList, jcinameList, descList, unitList, bacnettypeList))])\n",
    "                #for dataList in zip(nameList, jcinameList, unitList, bacnettypeList)]\n",
    "                #for dataList in zip(nameList, jcinameList, unitList)]\n",
    "                #for dataList in zip(nameList, jcinameList)]\n",
    "rawSentenceDict = dict([(i,reduce(adder, map(splitter, dataList)))\\\n",
    "                for i,dataList in enumerate(zip(rawNameList, rawJcinameList, rawDescList, unitList, bacnettypeList))])\n",
    "                #for dataList in zip(rawNameList, rawJcinameList, unitList, bacnettypeList)]\n",
    "                #for dataList in zip(rawNameList, rawJcinameList, unitList)]\n",
    "                #for dataList in zip(rawNameList, rawJcinameList)]\n",
    "\n",
    "# Make type, equip, location sentences\n",
    "equipSentenceList = list()\n",
    "pointSentenceList = list()\n",
    "locationSentenceList = list()\n",
    "\n",
    "for i, sentence in sentenceDict.items():\n",
    "    equipSentence = list()\n",
    "    pointSentence = list()\n",
    "    locationSentence = list()\n",
    "    for word in sentence:\n",
    "        closestTagList = scoreDictDict[word].keys()[0].split()\n",
    "        if len(closestTagList)==0:\n",
    "            continue\n",
    "        if sum([1 if tag in equipTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            equipSentence.append(word)\n",
    "        if sum([1 if tag in pointTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            pointSentence.append(word)\n",
    "        if sum([1 if tag in locationTagList else 0 for tag in closestTagList]) / float(len(closestTagList)) >=0.3:\n",
    "            locationSentence.append(word)\n",
    "    equipSentenceList.append(equipSentence)\n",
    "    pointSentenceList.append(pointSentence)\n",
    "    locationSentenceList.append(locationSentence)\n",
    "\n",
    "\n",
    "origSentenceDict = deepcopy(sentenceDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentenceDict = dict((idx,sentence) for idx,sentence in origSentenceDict.items() if idx in idxList)\n",
    "#sentenceDict = dict((idx, origSentenceDict[idx]) for idx in idxList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stpt\n",
      "rm\n",
      "trunk\n",
      "ap\n",
      "nae\n",
      "==========\n",
      "total default asking:  5\n"
     ]
    }
   ],
   "source": [
    "### Initial Human Input\n",
    "\n",
    "maxScoreDict = calc_max_score_dict(scoreDictDict)\n",
    "\n",
    "adder = lambda x,y:x+y\n",
    "wordList = list(set(reduce(adder,origSentenceDict.values(), [])))\n",
    "i = 0\n",
    "for word in wordList:\n",
    "    if sum([1 if word in sentence else 0 for sentence in origSentenceDict.values()])>len(origSentenceDict)*0.2 and len(word)>1:\n",
    "        if len(scoreDictDict[word].values())>1:\n",
    "            print word\n",
    "            i += 1\n",
    "print '=========='\n",
    "print 'total default asking: ', i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if buildingName=='bonner':\n",
    "    scoreDictDict['th'] = {'':1}\n",
    "    scoreDictDict['flr'] = {'':1}\n",
    "    scoreDictDict['vma'] = {'':1}\n",
    "    scoreDictDict['rm'] = {'room':1}\n",
    "    scoreDictDict['fc'] = {'':1}\n",
    "    scoreDictDict['bonner'] = {'':1}\n",
    "    scoreDictDict['sav'] = {'vav':1}\n",
    "    scoreDictDict['ah'] = {'ahu':1}\n",
    "    scoreDictDict['nae'] = {'':1}\n",
    "    scoreDictDict['nd'] = {'':1}\n",
    "elif buildingName=='ebu3b':\n",
    "    scoreDictDict['ebu'] = {'':1}\n",
    "    scoreDictDict['vma'] = {'':1}\n",
    "    scoreDictDict['nae'] = {'':1}\n",
    "    scoreDictDict['rm'] = {'room':1}\n",
    "elif buildingName=='ap_m':\n",
    "    scoreDictDict['ap'] = {'':1}\n",
    "    scoreDictDict['trunk'] = {'':1}\n",
    "    scoreDictDict['rm'] = {'room':1}\n",
    "    scoreDictDict['stpt'] = {'setpoint':1}\n",
    "    scoreDictDict['nae'] = {'':1}\n",
    "\n",
    "\n",
    "#scoreDictDict['actual'] = {'effective':1}\n",
    "#scoreDictDict['percent'] = {'':1}\n",
    "\n",
    "maxScoreDict = calc_max_score_dict(scoreDictDict)\n",
    "\n",
    "origScoreDictDict = deepcopy(scoreDictDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_tfidf_sub(cntDict, idfDict):\n",
    "    tfidfList = list()\n",
    "    maxCnt = max(cntDict.values())\n",
    "    for tag, idf in idfDict.items():\n",
    "        if not tag in cntDict.keys():\n",
    "            tfidf = 0\n",
    "        else:\n",
    "            tfidf = (0.5 + 0.5 * cntDict[tag] / maxCnt)* idf\n",
    "        tfidfList.append(tfidf)\n",
    "    return np.asarray(tfidfList)\n",
    "\n",
    "def sentence_tagset_score_segmented_tfidf(sentence, targetTagset):\n",
    "    targetTagset = targetTagset.split('_')\n",
    "    compDataLen = 5\n",
    "    sentenceCntDict = defaultdict(float)\n",
    "    \n",
    "    for word in sentence:\n",
    "        totalWordScore = 1.0/len(sentence)\n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:compDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values())) #* len(sentence)\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTagList = candTagset.split()\n",
    "            candTagScore = score / localScoreSum / len(candTagList)\n",
    "            for candTag in candTagList:\n",
    "                sentenceCntDict[candTag] += candTagScore\n",
    "                \n",
    "    tagsetCntDict = defaultdict(float)\n",
    "    #avgCnt = np.mean(sentenceCntDict.values())\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] += 1.0/len(targetTagset)\n",
    "        \n",
    "    entireCntDict = OrderedDict(sentenceCntDict)\n",
    "    for tag in targetTagset:\n",
    "        if tag not in sentenceCntDict.keys():\n",
    "            entireCntDict[tag] = tagsetCntDict[tag]\n",
    "    \n",
    "    entireCntDict = OrderedDict(entireCntDict)\n",
    "    totalCnt = sum(entireCntDict.values())\n",
    "    idfValues = [np.log(totalCnt/cnt) for cnt in entireCntDict.values()]\n",
    "    idfDict = OrderedDict()\n",
    "    for tag, idf in zip(entireCntDict.keys(), idfValues):\n",
    "        idfDict[tag]  = idf\n",
    "    \n",
    "    sentenceTfidf = calc_tfidf_sub(sentenceCntDict, idfDict)\n",
    "    tagsetTfidf = calc_tfidf_sub(tagsetCntDict, idfDict)\n",
    "    return 1 - cosine_similarity(sentenceTfidf, tagsetTfidf)\n",
    "\n",
    "#sentence_tagset_score = sentence_tagset_score_segmented_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maybe need log-scale compensation of redundant tags or words\n",
    "\n",
    "needDataLen =12\n",
    "\n",
    "def sentence_tagset_score_word_and_sentence_together(sentence, targetTagset, scoreDictDict=scoreDictDict):\n",
    "    origSentence = sentence\n",
    "    if type(targetTagset)==str or type(targetTagset)==unicode:\n",
    "        targetTagset = targetTagset.split('_')\n",
    "    \n",
    "    totalScore = 0 \n",
    "    notUsedWordCntDict = dict()\n",
    "    for word in sentence:\n",
    "        notUsedWordCntDict[word] = 0\n",
    "        \n",
    "    tagsetCntDict = dict()\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:needDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values()))\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTags = candTagset.split()\n",
    "            for candTag in candTags:\n",
    "                candTagScore = score / localScoreSum / len(candTags)\n",
    "                if candTag in targetTagset:\n",
    "                    tagsetCntDict[candTag] += candTagScore\n",
    "                else:\n",
    "                    notUsedWordCntDict[word] += candTagScore\n",
    "    tagsetScoreDict = dict()\n",
    "    for word, cnt in tagsetCntDict.items():\n",
    "        tagsetScoreDict[word] = np.log(1+cnt)\n",
    "    return (len(sentence)-sum(notUsedWordCntDict.values()))/len(sentence) * sum(tagsetScoreDict.values()) / len(targetTagset)\n",
    "    \n",
    "sentence_tagset_score = sentence_tagset_score_word_and_sentence_together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sentence_tagset_score_with_weight_inside(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, \\\n",
    "                                             maxScoreDict=maxScoreDict, totalScore=0, sentenceLen=None):\n",
    "    if type(targetTagset)==str or type(targetTagset)==unicode:\n",
    "        targetTagset = targetTagset.split('_')\n",
    "    \n",
    "    #TODO: Choose between those two.\n",
    "    if sentenceLen==None:\n",
    "        sentenceLen = len(sentence)\n",
    "    \n",
    "    totalScoreForTagset = 0\n",
    "    for word in sentence:\n",
    "        totalScoreForTagset += weightDict[word]\n",
    "    \n",
    "    try:\n",
    "        assert_almost_equal(totalScoreForTagset,0)\n",
    "        return 0, 0, 0    \n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    for word in deepcopy(sentence):\n",
    "        if len(word)<=1:\n",
    "            sentence.remove(word)\n",
    "    \n",
    "    notUsedWordCntDict = dict()\n",
    "    for word in sentence:\n",
    "        notUsedWordCntDict[word] = 0\n",
    "        \n",
    "    tagsetCntDict = dict()\n",
    "    for tag in targetTagset:\n",
    "        tagsetCntDict[tag] = 0\n",
    "        \n",
    "    sysWordUsageDict = defaultdict(float)\n",
    "    \n",
    "    for word in sentence:\n",
    "        wordScore = weightDict[word] * max(scoreDictDict[word].values()) / maxScoreDict[word]\n",
    "        \n",
    "        if wordScore==0:\n",
    "            continue\n",
    "            \n",
    "        scoreDict = dict(scoreDictDict[word].items()[0:needDataLen])\n",
    "        localScoreSum = float(sum(scoreDict.values()))\n",
    "        for candTagset, score in scoreDict.items():\n",
    "            candTags = candTagset.split()\n",
    "            for candTag in candTags:\n",
    "                candTagScore = score / localScoreSum / len(candTags) * wordScore\n",
    "                if candTag in targetTagset:\n",
    "                    tagsetCntDict[candTag] += candTagScore\n",
    "                else:\n",
    "                    notUsedWordCntDict[word] += candTagScore\n",
    "    tagsetScoreDict = dict()\n",
    "    for tag, cnt in tagsetCntDict.items():\n",
    "        tagsetScoreDict[tag] = np.log(1+cnt)\n",
    "    #for tag, cnt in tagsetCntDict.items():\n",
    "    #    tagsetScoreDict[tag] = cnt / (totalScoreForTagset/float(len(targetTagset)))\n",
    "    \n",
    "    \n",
    "    notUsedWordScoreDict = dict()\n",
    "    for word, cnt in notUsedWordCntDict.items():\n",
    "        notUsedWordScoreDict[word] = np.log(1+cnt)\n",
    "    #print \"notusedscore: \", notUsedWordScoreDict['chwp']\n",
    "        \n",
    "    \n",
    "    \n",
    "    sentenceUsage = sum(tagsetCntDict.values()) / float(sentenceLen)\n",
    "    tagsetCoverage = sum(tagsetScoreDict.values()) / (len(targetTagset)*np.log(1+totalScoreForTagset/float(len(targetTagset))))\n",
    "    #tagsetCoverage = sum(tagsetCntDict.values()) / totalScoreForTagset\n",
    "    candScore = sentenceUsage  * tagsetCoverage\n",
    "    \n",
    "    #print \"---\", targetTagset\n",
    "    #print \"totalScoreForTagset\", totalScoreForTagset\n",
    "    #print \"len(targetTagset)\", len(targetTagset)\n",
    "    #print \"---\"\n",
    "    \n",
    "    \n",
    "    #print sentenceUsage\n",
    "    #print tagsetCoverage\n",
    "    #pp.pprint(tagsetScoreDict)\n",
    "    \n",
    "    \n",
    "    return candScore, sentenceUsage, tagsetCoverage#, notUsedWordCntDict\n",
    "    \n",
    "    \n",
    "def sentence_tagset_score_with_weight(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict, totalScore=0):\n",
    "    candScore, _, _ = sentence_tagset_score_with_weight_inside(sentence, targetTagset, weightDict, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict, totalScore=0)\n",
    "    return candScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_score_altogether_in_sentence(comb, candDict, scoreDictDict):\n",
    "    sentence = comb.get_sentence()\n",
    "    #TODO: Check which is good between below two lines\n",
    "    #sentenceTotalScore = len(sentence) - sum(comb.get_weight_dict('dummy').values())\n",
    "    sentenceTotalScore = len(sentence)\n",
    "    sentenceUsageSum = 0\n",
    "    tagsetCoverageList = list()\n",
    "    weightSumList = list()\n",
    "    candScoreSum = 0\n",
    "    #TODO: Choose to include this or not\n",
    "    #sentenceLen = len(sentence) - sum(comb.get_weight_dict('dummy').values())\n",
    "    sentenceLen = None\n",
    "    for tagsetType, cand in candDict.items():\n",
    "        # TODO: Return to the original\n",
    "        candScore, sentenceUsage, tagsetCoverage = sentence_tagset_score_with_weight_inside(sentence, cand, \\\n",
    "                                        comb.get_weight_dict(tagsetType), scoreDictDict, sentenceLen=sentenceLen)\n",
    "        sentenceUsageSum += sentenceUsage\n",
    "        tagsetCoverageList.append(tagsetCoverage)\n",
    "        weightSumList.append(sum(comb.get_weight_dict(tagsetType).values())) \n",
    "        candScoreSum += candScore\n",
    "        #print cand, candScore, sentenceUsage, tagsetCoverage\n",
    "    #sentenceUsage = sentenceUsageSum / sentenceTotalScore\n",
    "    #totalTagsetCoverage = sum([cov*weight for cov, weight in zip(tagsetCoverageList, weightSumList)])/float(sum(weightSumList))\n",
    "    return candScoreSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractSubDict = lambda origDict, tagsetTypeList: \\\n",
    "            dict((tagsetType, subTagList) for tagsetType, subTagList in origDict.items() if tagsetType in tagsetTypeList)\n",
    "subTagListDictList = [extractSubDict(subTagListDict, ['point']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'equip']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'equip', 'location']),\\\n",
    "                      extractSubDict(subTagListDict, ['point', 'location'])\n",
    "                      ]\n",
    "                      \n",
    "scoreDictDict = dict()\n",
    "\n",
    "\n",
    "\n",
    "for idx, sentence in sentenceDict.items():    \n",
    "    for word in sentence:\n",
    "        if word not in scoreDictDict.keys():\n",
    "            scoreDictDict[word] = origScoreDictDict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_comb((sentence, subTagListDictList, scoreDictDict, reduceCandNum)):\n",
    "    combList = list()\n",
    "    for tagListDict in subTagListDictList:\n",
    "        tagTypeList = tagListDict.keys()\n",
    "        comb = SentenceTagsetsCombination(sentence, tagListDict, scoreDictDict)\n",
    "        sentenceTotalScore = len(comb.get_sentence()) - sum(comb.get_weight_dict('dummy').values())\n",
    "        weightDictDict = comb.get_total_weight_dict()\n",
    "        candDictList = list()\n",
    "        multipleTagsetScoreDict = defaultdict(dict)\n",
    "        for tagsetType, weightDict in weightDictDict.items():\n",
    "            if tagsetType=='dummy':\n",
    "                continue\n",
    "            tagsetList = subTagsetListDict[tagsetType]\n",
    "            for tagset in tagsetList:\n",
    "                #multipleTagsetScoreDict[tagsetType][tagset] = sentence_tagset_score_with_weight(sentence, \\\n",
    "#                                                                                                tagset, weightDict)\n",
    "                multipleTagsetScoreDict[tagsetType][tagset] = calc_score_altogether_in_sentence(comb, {tagsetType:tagset}, \\\n",
    "                                                                                                scoreDictDict)\n",
    "        multipleTagsetScoreDict = dict(multipleTagsetScoreDict)\n",
    "        \n",
    "        pointItemList = list()\n",
    "        equipPointItemList = list()\n",
    "        \n",
    "        pointCandNum = 5 - reduceCandNum\n",
    "        if pointCandNum<1:\n",
    "            pointCandNum = 1\n",
    "        \n",
    "        pointEquipCandNum = 5 - reduceCandNum\n",
    "        if pointEquipCandNum<2:\n",
    "            pointEquipCandNum = 2\n",
    "        locationCandNum = 3 - reduceCandNum\n",
    "        if locationCandNum <2:\n",
    "            locationcandNum = 2\n",
    "        \n",
    "        chosenPointTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['point'], pointCandNum)\n",
    "        #chosenItemList \n",
    "        pointItemList += [({'point':pointTagset}, score) for pointTagset, score in chosenPointTagsetDict.items()]\n",
    "        \n",
    "        equipPointItemList = list()\n",
    "        \n",
    "        if 'equip' in tagTypeList:\n",
    "            chosenEquipTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['equip'], pointEquipCandNum)\n",
    "            for equipTagset, equipScore in chosenEquipTagsetDict.items():\n",
    "                for pointTagset, pointScore in chosenPointTagsetDict.items():\n",
    "                    if pointTagset in equipPointDict[equipTagset]:\n",
    "                        candDict = {'point': pointTagset, 'equip':equipTagset}\n",
    "                        equipPointItemList.append((candDict, calc_score_altogether_in_sentence(comb, candDict, scoreDictDict)))\n",
    "        '''\n",
    "        if 'equip' in tagTypeList:\n",
    "            chosenEquipTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['equip'], 4 - reduceCandNum)\n",
    "            for equipTagset, equipScore in chosenEquipTagsetDict.items():\n",
    "                for pointTagset, pointScore in chosenPointTagsetDict.items():\n",
    "                    candDict = {'point': pointTagset, 'equip':equipTagset}\n",
    "                    equipPointItemList.append((candDict, calc_score_altogether_in_sentence(comb, candDict, scoreDictDict)))\n",
    "        '''\n",
    "        \n",
    "        if 'location' in tagTypeList:\n",
    "            chosenLocationTagsetDict = pick_dict_with_best_n_from_dict(multipleTagsetScoreDict['location'], locationCandNum)\n",
    "            locationEquipPointItemList = list()\n",
    "            locationPointItemList = list()\n",
    "            for locationTagset, locationScore in chosenLocationTagsetDict.items():\n",
    "                if 'equip' in tagTypeList:\n",
    "                    for item in equipPointItemList:\n",
    "                        item[0]['location'] = locationTagset\n",
    "                        locationEquipPointItemList.append((deepcopy(item[0]), item[1]+locationScore))\n",
    "                else:\n",
    "                    for item in pointItemList:\n",
    "                        item[0]['location'] = locationTagset\n",
    "                        locationPointItemList.append((deepcopy(item[0]), item[1]+locationScore))\n",
    "                    \n",
    "                    \n",
    "        if 'equip' in tagTypeList:\n",
    "            if 'location' in tagTypeList:\n",
    "                chosenItemList = locationEquipPointItemList\n",
    "            else:\n",
    "                chosenItemList = equipPointItemList\n",
    "        elif 'location' in tagTypeList:\n",
    "            chosenItemList = locationPointItemList\n",
    "        else:\n",
    "            chosenItemList = pointItemList\n",
    "        for item in chosenItemList:\n",
    "            tempComb = deepcopy(comb)\n",
    "            tempComb.set_cand_set(item[0], item[1])\n",
    "            combList.append(tempComb)\n",
    "    return combList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "beginTime =  datetime.now()\n",
    "if mpFlag:\n",
    "    pool = multiprocessing.Pool(workerNum)\n",
    "    combListList = pool.map(init_comb, zip(sentenceDict.values(), repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "    pool.terminate()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "else:\n",
    "    #combListList = map(init_comb, zip(sentenceList, repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "    combListList = list()\n",
    "    for i, sentence in enumerate(sentenceDict.values()):\n",
    "        combListList.append(init_comb((sentence, subTagListDictList, scoreDictDict, 0)))\n",
    "    pass\n",
    "combListDict = dict((idx,combList) for idx, combList in zip(idxList, combListList))\n",
    "endTime = datetime.now()\n",
    "\n",
    "origCombListDict = deepcopy(combListDict)\n",
    "\n",
    "print (endTime-beginTime).total_seconds()/60, 'minutes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_comb_cand_dict_score(combListDict, scoreDictDict):\n",
    "    for idx, combList in combListDict.items():\n",
    "        for comb in combList:\n",
    "            score = calc_score_altogether_in_sentence(comb, comb.get_cand_set(), scoreDictDict)\n",
    "            comb.set_score(score)\n",
    "    return combListDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calc_building_score(combListDict):\n",
    "    buildingScore = 0\n",
    "    scoreGetter = lambda x: x.get_score()\n",
    "    for idx, combList in combListDict.items():\n",
    "        scoreList = map(scoreGetter,combList)\n",
    "        scoreSum = sum(scoreList)\n",
    "        buildingScore += sum([score*score/scoreSum for score in scoreList])\n",
    "        #buildingScore += sum([score*score/scoreSum/scoreSum for score in scoreList])\n",
    "        #buildingScore += stats.entropy(scoreList)\n",
    "        \n",
    "    return buildingScore\n",
    "\n",
    "buildingScoreList = [calc_building_score(origCombListDict)]\n",
    "print buildingScoreList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval_word_meaning2((combListDict, word, scoreDictDict)):\n",
    "    scoreDict = scoreDictDict[word]\n",
    "    if len(scoreDict)<=1:\n",
    "        return None\n",
    "    baseCombListDict = dict()\n",
    "    for i, combList in combListDict.items():\n",
    "        if word in combList[0].get_sentence():\n",
    "            #baseCombListDict[i] = deepcopy(combList)\n",
    "            baseCombListDict[i] = combList\n",
    "            #TODO: Rollback if not works\n",
    "    if len(baseCombListDict)<1:\n",
    "        return None\n",
    "    baseBuildingScore = calc_building_score(baseCombListDict)\n",
    "    \n",
    "    removeCandList = scoreDict.keys()\n",
    "    #updatedCombListDictList = list()\n",
    "    updatedScoreList = list()\n",
    "    for removeCand in removeCandList:\n",
    "        updatedScoreDictDict = deepcopy(scoreDictDict)\n",
    "        del updatedScoreDictDict[word][removeCand]\n",
    "        \n",
    "        #combListDict = deepcopy(baseCombListDict)\n",
    "        \n",
    "        #updatedCombListDictList.append(update_comb_cand_dict_score(combListDict, scoreDictDict=updatedScoreDictDict))\n",
    "        #updatedCombListDict = update_comb_cand_dict_score(combListDict, scoreDictDict=updatedScoreDictDict)\n",
    "        updatedCombListDict = update_comb_cand_dict_score(baseCombListDict, scoreDictDict=updatedScoreDictDict)\n",
    "        updatedScoreList.append(calc_building_score(updatedCombListDict))\n",
    "        combListDict = None\n",
    "        updatedCombListDict = None\n",
    "    \n",
    "    #for updatedCombListDict in updatedCombListDictList:\n",
    "    #    updatedScoreList.append(calc_building_score(updatedCombListDict))\n",
    "    \n",
    "    baseCombListDict = None\n",
    "    combListDict = None\n",
    "    updatedScoreDictDict = None\n",
    "    if max(updatedScoreList) > baseBuildingScore:\n",
    "        selectedScore = max([score for score in updatedScoreList if [score - baseBuildingScore]>0])\n",
    "        removeTagset = removeCandList[updatedScoreList.index(selectedScore)]\n",
    "        return (word, removeTagset)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "wordUpdateIterationNum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def try_add_weight(word, targetTagsetType, totalWeightDict, delta=0.1):\n",
    "    tagsetTypeTotalNum = len(totalWeightDict)\n",
    "    totalWeightDict[targetTagsetType][word] += delta\n",
    "    currWeight = totalWeightDict[targetTagsetType][word]\n",
    "    if currWeight>1:\n",
    "        delta -= currWeight - 1\n",
    "        totalWeightDict[targetTagsetType][word] = 1\n",
    "    returnWeight = 0\n",
    "    for tagsetType in totalWeightDict.keys():\n",
    "        if tagsetType==targetTagsetType:\n",
    "            continue\n",
    "        totalWeightDict[tagsetType][word] -= delta / float((tagsetTypeTotalNum-1))\n",
    "        if totalWeightDict[tagsetType][word]<0:\n",
    "            returnWeight += (0-totalWeightDict[tagsetType][word])\n",
    "            totalWeightDict[tagsetType][word] = 0\n",
    "    totalWeightDict[targetTagsetType][word] -= returnWeight\n",
    "    return totalWeightDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_weight_per_comb(comb, scoreDictDict):\n",
    "    delta = 0.1\n",
    "    candSet = comb.get_cand_set()\n",
    "    origTotalWeightDict = comb.get_total_weight_dict()\n",
    "    words = origTotalWeightDict.values()[0].keys()\n",
    "    foundTotalWeightDict = defaultdict(dict)\n",
    "    tagsetTypeTotalNum = len(origTotalWeightDict)\n",
    "    for word in words:\n",
    "        totalWeightDictCandDict = dict()\n",
    "        scoreDict = dict()\n",
    "        for targetTagsetType, weightDict in origTotalWeightDict.items():\n",
    "            totalWeightDict = try_add_weight(word, targetTagsetType, deepcopy(origTotalWeightDict), delta=delta)\n",
    "            score = calc_score_altogether_in_sentence(comb.set_total_weight_dict(totalWeightDict), candSet, scoreDictDict)\n",
    "            scoreDict[targetTagsetType] = score\n",
    "            totalWeightDictCandDict[targetTagsetType] = totalWeightDict\n",
    "        \n",
    "        maxScore = max(scoreDict.values())\n",
    "        maxIdx = scoreDict.values().index(maxScore)\n",
    "        maxTagsetType = scoreDict.keys()[maxIdx]\n",
    "        \n",
    "        chosenTotalWeightDict = totalWeightDictCandDict[maxTagsetType]\n",
    "        for tagsetType in totalWeightDict.keys():\n",
    "            foundTotalWeightDict[tagsetType][word] = chosenTotalWeightDict[tagsetType][word]\n",
    "    comb.set_total_weight_dict(foundTotalWeightDict)\n",
    "    comb.set_score(calc_score_altogether_in_sentence(comb, candSet, scoreDictDict))\n",
    "    return comb\n",
    "\n",
    "def eval_weight((combListDict, scoreDictDict)):\n",
    "    newCombListDict = defaultdict(list)\n",
    "    for idx, combList in combListDict.items():\n",
    "        for comb in combList:\n",
    "            newCombListDict[idx].append(eval_weight_per_comb(comb, scoreDictDict))\n",
    "    return dict(newCombListDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_weak_candidates(combListDict):\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for idx, combList in combListDict.items():\n",
    "        if len(combList)<4:\n",
    "            continue\n",
    "        scoreList = map(scoreGetter, combList)\n",
    "        minVal = min(scoreList)\n",
    "        minIdx = scoreList.index(minVal)\n",
    "        del combList[minIdx]\n",
    "        combListDict[idx] = combList\n",
    "    return combListDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Asking words' meaning to experts\n",
    "def change_word_meaning(word, meaning, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict):\n",
    "    scoreDictDict[word] = {meaning: maxScoreDict[word]}\n",
    "    return scoreDictDict\n",
    "\n",
    "def pick_weak_word_in_cluster(idxList, combListDict, scoreDictDict):\n",
    "    adder = lambda x,y:x+y\n",
    "    wordOccurrenceDict = dict()\n",
    "    wordUsageDict = dict()\n",
    "    occIdxDict = dict()\n",
    "    cnt = 0\n",
    "    wordEntropyDict = dict()\n",
    "    candSetGetter = lambda x:x.get_cand_set()\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for word, scoreDict in scoreDictDict.items():\n",
    "        wordEntropyDict[word] = stats.entropy(scoreDict.values())\n",
    "        cnt += 1\n",
    "        if cnt%100==0:\n",
    "            print cnt\n",
    "        occIndices = [idx for idx, sentence in zip(idxList, [sentenceDict[i] for i in idxList]) if word in sentence]\n",
    "        occIdxDict[word] = occIndices\n",
    "        wordOccurrence = len(occIndices)\n",
    "        wordOccurrenceDict[word] = wordOccurrence\n",
    "        if wordOccurrence==0 or len(scoreDict)==1:\n",
    "            continue\n",
    "        usageScore = 0\n",
    "        for idx in occIndices:\n",
    "            combList = combListDict[idx]\n",
    "            #candSetList = map(candSetGetter, combList)\n",
    "            totalScore = sum(map(scoreGetter, combList))\n",
    "            subScore = 0\n",
    "            wordCandScoreSum = sum(scoreDict.values())\n",
    "            for wordTagsetCand, tagsetScore in scoreDict.items():\n",
    "                wordTagsetCand = wordTagsetCand.split()\n",
    "                for tagCand in wordTagsetCand:\n",
    "                    for comb in combList:\n",
    "                        for cand in comb.get_cand_set().values():\n",
    "                            cand = cand.split('_')\n",
    "                            if tagCand in cand:\n",
    "                                subScore += tagsetScore/wordCandScoreSum/len(wordTagsetCand) * comb.get_score() / totalScore / len(cand)\n",
    "            usageScore += subScore\n",
    "        wordUsageDict[word] = usageScore\n",
    "\n",
    "    percentUsageDict = dict([(word, wordUsageDict[word]/wordOccurrenceDict[word]) for word in wordUsageDict.keys()])\n",
    "\n",
    "    sortedUsage = sorted(percentUsageDict.values())\n",
    "    '''print \"========\"\n",
    "    for word, usage in sorted(percentUsageDict.items(), key=itemgetter(1)):\n",
    "        if wordOccurrenceDict[word]>2 and len(word)>1:\n",
    "            print word, usage, wordOccurrenceDict[word], wordEntropyDict[word]\n",
    "    print \"========\"\n",
    "    '''\n",
    "    return percentUsageDict\n",
    "\n",
    "#percentUsageDict = pick_weak_word_in_cluster(range(0,coverageNum), combListDict, scoreDictDict)\n",
    "\n",
    "\n",
    "def ask_experts(bestCandScoreDict, combListDict, buildingIdxList, scoreDictDict=scoreDictDict, maxScoreDict=maxScoreDict):\n",
    "    answerCnt = 0\n",
    "    \n",
    "    percentUsageDict = pick_weak_word_in_cluster(buildingIdxList, combListDict, scoreDictDict)\n",
    "    \n",
    "    pointIdxDict = defaultdict(list)\n",
    "    for i, (bestComb, bestCand, bestScore) in bestCandScoreDict.items():\n",
    "        pointIdxDict[bestCand['point']].append(i)\n",
    "    avgScorePerPointDict = dict()\n",
    "    for key, idxList in pointIdxDict.items():\n",
    "        avgScorePerPointDict[key] = np.mean([cand[2] for cand in [bestCandScoreDict[i] for i in idxList]])\n",
    "    for key, mean in sorted(avgScorePerPointDict.items(), key=itemgetter(1)):\n",
    "        #print key, mean, len(pointIdxDict[key])\n",
    "        pass\n",
    "\n",
    "    lenGetter = lambda x: len(x[1])\n",
    "    totalScoreGetter = lambda (key,avg):avg*len(pointIdxDict[key])\n",
    "\n",
    "    weakestPointList = list()\n",
    "    #for key, idxList in sorted(pointIdxDict.items(), key=lenGetter, reverse=True):\n",
    "    #    print key, len(pointIdxDict[key])\n",
    "    for key, avg in sorted(avgScorePerPointDict.items(), key=totalScoreGetter):\n",
    "        num = len(pointIdxDict[key])\n",
    "        if num>3:\n",
    "            weakestPointList.append(key)\n",
    "            \n",
    "    #print \"=================score baesd asking\"\n",
    "    #for pointTagset in weakestPointList[0:15]:\n",
    "    #    print pick_weak_word_in_cluster(pointIdxDict[pointTagset], candListDict, scoreDictDict)\n",
    "    #print \"\\n\"\n",
    "        \n",
    "        \n",
    "    commonWordListDict = defaultdict(set)\n",
    "    adder = lambda x,y:x+y\n",
    "    for pointTagset, idxList in pointIdxDict.items():        \n",
    "        subSentenceList = [sentenceDict[i] for i in idxList]\n",
    "        allWordList = list(set(reduce(adder, subSentenceList,[])))\n",
    "        for word in allWordList:\n",
    "            cnt = 0\n",
    "            for sentence in subSentenceList:\n",
    "                if word in sentence:\n",
    "                    cnt += 1\n",
    "                if cnt/float(len(idxList)) > 0.25:\n",
    "                    commonWordListDict[pointTagset].add(word)\n",
    "    \n",
    "    avgScoreGetter = lambda item: avgScorePerPointDict[item[0]]\n",
    "    commonWordListDict = OrderedDict(sorted(commonWordListDict.items(), key=avgScoreGetter))\n",
    "    \n",
    "    shownWordList = list()\n",
    "    anchorListDict = defaultdict(list)\n",
    "    for pointTagset, commonWordList in commonWordListDict.items():\n",
    "        if pointIdxDict[pointTagset]<5:\n",
    "            continue\n",
    "        for word in commonWordList:\n",
    "            if word not in shownWordList:\n",
    "                if word in percentUsageDict.keys() and len(word)>1:\n",
    "                    print word, percentUsageDict[word]\n",
    "                    shownWordList.append(word)\n",
    "                    anchorListDict[pointTagset].append((word, percentUsageDict[word]))\n",
    "    \n",
    "    itemNumGetter = lambda x:len(pointIdxDict[x[0]])\n",
    "\n",
    "    for pointTagset, anchorList in sorted(anchorListDict.items(), key=itemNumGetter, reverse=True):\n",
    "        print pointTagset\n",
    "        print '\\t', sorted(anchorList, key=itemgetter(1))\n",
    "        \n",
    "    resp = ''\n",
    "    while resp!='q':\n",
    "        resp = raw_input('''\n",
    "if you want to fix words, put a word to fix.\n",
    "stop asking for now and init again?\\t q\n",
    "stop asking for now but no init?\\t qq\n",
    "stop asking forever and no init?\\t qqq\n",
    "                            ''')\n",
    "        if resp=='q':\n",
    "            pass\n",
    "        elif resp=='qq':\n",
    "            return True, False, answerCnt\n",
    "        elif resp=='qqq':\n",
    "            return False, False, answerCnt\n",
    "        else:\n",
    "            word = resp\n",
    "            if word not in scoreDictDict.keys():\n",
    "                print \"What was the word again?\"\n",
    "                continue\n",
    "            meaning = raw_input(\"meaning?\\n\")\n",
    "            change_word_meaning(word, meaning, scoreDictDict)\n",
    "            answerCnt += 1\n",
    "    return True, True, answerCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_best_cand_dict(combListDict):\n",
    "    bestCandScoreDict = dict()\n",
    "    scoreGetter = lambda x:x.get_score()\n",
    "    for i, combList in combListDict.items():\n",
    "        scoreList = map(scoreGetter, combList)\n",
    "        maxVal = max(scoreList)\n",
    "        maxIdx = scoreList.index(maxVal)\n",
    "        bestComb = combList[maxIdx]\n",
    "        bestCandScoreDict[i] = (bestComb, bestComb.get_cand_set(), maxVal)\n",
    "    return bestCandScoreDict\n",
    "\n",
    "def check_correct(predLabel, currLabel):\n",
    "    currLabel = currLabel.replace(' ', '_')\n",
    "    possibleLabelList = [currLabel]\n",
    "    if 'supply' in currLabel:\n",
    "        possibleLabelList.append(currLabel.replace('supply', 'discharge'))\n",
    "    if 'discharge' in currLabel:\n",
    "        possibleLabelList.append(currLabel.replace('discharge', 'supply'))\n",
    "    if currLabel in equalDict.keys():\n",
    "        possibleLabelList.append(equalDict[currLabel])\n",
    "    if predLabel in possibleLabelList:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def str_conv(d):\n",
    "    if type(d)==str or type(d)==unicode:\n",
    "        return d\n",
    "    else:\n",
    "        if np.isnan(d):\n",
    "            return ''\n",
    "        else:\n",
    "            return str(d)\n",
    "        \n",
    "def eval_result(bestCandScoreDict, randIdxList, trueDF):\n",
    "    cnt = 0\n",
    "    totalCntDict = defaultdict(int)\n",
    "    correctCntDict = defaultdict(int)\n",
    "    incorrectCntDict = defaultdict(int)\n",
    "    unknownDict = defaultdict(int)\n",
    "    randomSelectedDict = dict()\n",
    "    for i, (comb, candSet, candSetScore) in bestCandScoreDict.items():\n",
    "        if i in randIdxList:\n",
    "            cnt += 1\n",
    "            srcid = sensorDF.iloc[i].name\n",
    "            randomSelectedDict[i] = comb\n",
    "            brickTagsetDict = OrderedDict()\n",
    "            brickTagsetDict['point'] = str_conv(trueDF.loc[srcid]['BRICK_POINT']).split(',')\n",
    "            brickTagsetDict['equip'] = str_conv(trueDF.loc[srcid]['BRICK_EQUIP_LIST']).split(',')\n",
    "            brickTagsetDict['location'] = str_conv(trueDF.loc[srcid]['BRICK_LOCATION_LIST']).split(',')\n",
    "            \n",
    "            for tagsetType, brickTagsetList in brickTagsetDict.items():\n",
    "                if brickTagsetList[0]=='':\n",
    "                    unknownDict[tagsetType] += 1\n",
    "                elif tagsetType in candSet.keys():\n",
    "                    #if tagsetType=='point' and i==2100:\n",
    "                    #    print '------------'\n",
    "                    #    print map(checkCorrectMapper, brickTagsetList)\n",
    "                    #    print '------------'\n",
    "                    \n",
    "                    checkCorrectMapper = lambda aTagset:check_correct(candSet[tagsetType], aTagset)\n",
    "                    if True in map(checkCorrectMapper, brickTagsetList):\n",
    "                        correctCntDict[tagsetType] += 1\n",
    "                    else:\n",
    "                        incorrectCntDict[tagsetType] += 1\n",
    "                    totalCntDict[tagsetType] += len(brickTagsetList)\n",
    "                else:\n",
    "                    totalCntDict[tagsetType] += len(brickTagsetList)\n",
    "    recallDict = dict()\n",
    "    precisionDict = dict()\n",
    "    unknownRatioDict = dict()\n",
    "    for tagsetType, correctCnt in correctCntDict.items():\n",
    "        recallDict[tagsetType] = correctCnt / float(totalCntDict[tagsetType])\n",
    "        precisionDict[tagsetType] = correctCnt / float(correctCnt+incorrectCntDict[tagsetType])\n",
    "        unknownRatioDict[tagsetType] = unknownDict[tagsetType]/float(cnt)\n",
    "    resultDict={'precision':precisionDict,\\\n",
    "               'recall':recallDict,\n",
    "               'unknownRatioDict':unknownRatioDict,\n",
    "                'sampleDict':randomSelectedDict\n",
    "               }\n",
    "    return resultDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordUpdateIterationNum = 0\n",
    "combListDict = deepcopy(origCombListDict)\n",
    "scoreDictDict = deepcopy(origScoreDictDict)\n",
    "buildingScoreList = [calc_building_score(combListDict)]\n",
    "resultHistory = list()\n",
    "answerCnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iterNum = 10\n",
    "\n",
    "askFlag = True\n",
    "initFlag = True\n",
    "reduceCandNum = 0\n",
    "for iterCnt in range(0,iterNum):\n",
    "    print wordUpdateIterationNum\n",
    "\n",
    "    if initFlag:\n",
    "        print \"INITED\"\n",
    "        ######################### Init Cand \n",
    "        beginTime =  datetime.now()\n",
    "        if mpFlag:\n",
    "            pool = multiprocessing.Pool(workerNum)\n",
    "            combListList = pool.map(init_comb, zip(sentenceDict.values(), repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "            pool.terminate()\n",
    "            pool.close()\n",
    "            pool.join()\n",
    "        else:\n",
    "            combListList = map(init_comb, zip(sentenceDict.values(), repeat(subTagListDictList), repeat(scoreDictDict), repeat(0)))\n",
    "            pass\n",
    "        combListDict = dict((i,combList) for i, combList in zip(sentenceDict.keys(),combListList))\n",
    "        endTime = datetime.now()\n",
    "        print \"comb init time:\", (endTime-beginTime).total_seconds()/60, 'minutes'\n",
    "\n",
    "\n",
    "    #print iterCnt\n",
    "    ########################## Update Weights\n",
    "    beginTime =  datetime.now()\n",
    "    makeDict = lambda x:dict([x])\n",
    "    adder = lambda x,y:x+y\n",
    "    dictgetter = lambda x:x.items()\n",
    "    if mpFlag:\n",
    "        pool = multiprocessing.Pool(workerNum)\n",
    "        updatedDictList = pool.map(eval_weight, zip(map(makeDict,combListDict.items()), repeat(scoreDictDict)))\n",
    "        pool.terminate()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        pass\n",
    "    else:\n",
    "        updatedDictList = map(eval_weight, zip(map(makeDict,combListDict.items()), repeat(scoreDictDict)))\n",
    "    combListDict = dict(reduce(adder,map(dictgetter, updatedDictList)))\n",
    "\n",
    "    endTime = datetime.now()\n",
    "    print \"weight computing time:\", (endTime-beginTime).total_seconds()/60, '(min)'\n",
    "\n",
    "\n",
    "    #################### Update Word Meaning\n",
    "    print \"init word update\"\n",
    "\n",
    "    baseBuildingScore = calc_building_score(combListDict)\n",
    "\n",
    "    beginTime =  datetime.now()\n",
    "    if mpFlag:\n",
    "        pool = multiprocessing.Pool(workerNum)\n",
    "        removeWordTagsetTupleList = pool.map(eval_word_meaning2, zip(repeat(combListDict), scoreDictDict.keys(), \\\n",
    "                                                                     repeat(scoreDictDict)))\n",
    "        pool.terminate()\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        removeWordTagsetTupleList = map(eval_word_meaning2, zip(repeat(combListDict), scoreDictDict.keys(), repeat(scoreDictDict)))\n",
    "    removeWordTagsetDict = dict([el for el in removeWordTagsetTupleList if el])\n",
    "    endTime = datetime.now()\n",
    "\n",
    "    #update removing word meanings\n",
    "    for word, tagset in removeWordTagsetDict.items():\n",
    "        del scoreDictDict[word][tagset]\n",
    "\n",
    "    combListDict = update_comb_cand_dict_score(combListDict, scoreDictDict)\n",
    "    buildingScoreList.append(calc_building_score(combListDict))\n",
    "    wordUpdateIterationNum += 1\n",
    "\n",
    "\n",
    "    ################## Remove weak candidates\n",
    "    combListDict = remove_weak_candidates(combListDict)\n",
    "    #combListDict = remove_weak_candidates(combListDict)\n",
    "    print \"word meaning computing time:\", (endTime-beginTime).total_seconds()/60, '(min)'\n",
    "    print buildingScoreList\n",
    "    print \"================\"\n",
    "\n",
    "    ########## Calculate best candidates\n",
    "    bestCandScoreDict = calc_best_cand_dict(combListDict)\n",
    "    #scoreGetter = lambda x:x.get_score()\n",
    "    #for i, combList in combListDict.items():\n",
    "    #    scoreList = map(scoreGetter, combList)\n",
    "    #    maxVal = max(scoreList)\n",
    "    #    maxIdx = scoreList.index(maxVal)\n",
    "    #    bestComb = combList[maxIdx]\n",
    "    #    bestCandScoreDict[i] = (bestComb, bestComb.get_cand_set(), maxVal)\n",
    "\n",
    "    resultHistory.append(eval_result(bestCandScoreDict, randIdxList, trueDF))\n",
    "\n",
    "\n",
    "    if askFlag:\n",
    "        askFlag, initFlag, answerNum = ask_experts(bestCandScoreDict, combListDict, idxList, scoreDictDict)\n",
    "        answerCnt += answerNum\n",
    "\n",
    "print \"Human Inputs: \", answerCnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = resultHistory[-1]\n",
    "sampleDict = result['sampleDict']\n",
    "for i, comb in sampleDict.items():\n",
    "    print i, comb.get_sentence(), comb.get_cand_set()\n",
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in sampleDict.keys():\n",
    "    srcid = sensorDF.index[i]\n",
    "    brickPoint = str_conv(trueDF.loc[srcid]['BRICK_POINT']).split(',')\n",
    "    tagset = resultHistory[-1]['sampleDict'][i].get_cand_set()['point']\n",
    "    print i, brickPoint[0]\n",
    "    print tagset\n",
    "    #print tagset in brickPoint\n",
    "    print check_correct(str_conv(tagset), brickPoint[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "precList = list()\n",
    "for result in resultHistory:\n",
    "    precList.append(result['precision']['point'])\n",
    "plt.plot(precList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "precList = list()\n",
    "for result in resultHistory[0:5]:\n",
    "    precList.append(result['recall']['equip'])\n",
    "plt.plot(precList)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'result/'+buildingName+'_comb_cand.csv'\n",
    "with open(filename, 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(['sentence', 'true point type', 'true equip type', 'point correct?', 'candidate combi', 'score', 'candidate combi', 'score'])\n",
    "    addFunc = lambda x,y:x+y\n",
    "\n",
    "    candScoreGetter = lambda x:(x.get_cand_set(), x.get_score())\n",
    "    \n",
    "    for i, combList in combListDict.items():\n",
    "        #candTupleList = reduce(addFunc,map(itemgetter(1), tagsetTypeBasedList),[])\n",
    "        #candTupleList = sorted(candTupleList, key=itemgetter(1), reverse=True)\n",
    "        candTupleList = map(candScoreGetter, combList)\n",
    "        candTupleList = sorted(candTupleList, key=itemgetter(1), reverse=True)\n",
    "        comb = combList[0]\n",
    "        if not buildingName in notUcsdBuildings:\n",
    "            srcid = sensorDF.index[i]\n",
    "            trueRow = trueDF.loc[srcid]\n",
    "            pointCorrect = check_correct(bestCandScoreDict[i][1]['point'], str_conv(trueRow['Schema Label']))\n",
    "            writeList = [comb.get_sentence(), trueRow['Schema Label'], trueRow['Equipment Type'], 1 if pointCorrect else 0]\n",
    "        else:\n",
    "            label = df.iloc[i]['Schema Label']\n",
    "            writeList = [comb.get_sentence(), label]\n",
    "        for candTuple in candTupleList:\n",
    "            writeList.append(candTuple[0])\n",
    "            writeList.append(candTuple[1])\n",
    "        writer.writerow(writeList)\n",
    "f = gdrive.CreateFile()\n",
    "f.SetContentFile(filename)\n",
    "f.Upload()\n",
    "f = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = 'result/'+buildingName+'_random_sample_result.csv'\n",
    "with open(filename, 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(['srcid', 'sentence', 'BRICK_POINT', 'estimated_point', 'point correct?', \\\n",
    "                     'BRICK_EQUIP', 'estimated_equip', 'equip correct?', \\\n",
    "                     'BRICK_LOCATION', 'estimated_loc', 'loc correct?', 'cand_set'])\n",
    "    for i, combList in combListDict.items():\n",
    "        rowStr = list()\n",
    "        if i in randIdxList:\n",
    "            srcid = sensorDF.iloc[i].name\n",
    "            rowStr.append(srcid)\n",
    "            (bestComb, bestCandSet, bestScore) = bestCandScoreDict[i]\n",
    "            rowStr.append(bestComb.get_sentence())\n",
    "            brickTagsetDict = OrderedDict()\n",
    "            brickTagsetDict['point'] = str_conv(trueDF.loc[srcid]['BRICK_POINT']).split(',')\n",
    "            brickTagsetDict['equip'] = str_conv(trueDF.loc[srcid]['BRICK_EQUIP_LIST']).split(',')\n",
    "            brickTagsetDict['location'] = str_conv(trueDF.loc[srcid]['BRICK_LOCATION_LIST']).split(',')\n",
    "            for tagsetType, correctTagsetList in brickTagsetDict.items():\n",
    "                rowStr.append(correctTagsetList)\n",
    "                if tagsetType in bestCandSet.keys():\n",
    "                    estimatedTagset = bestCandSet[tagsetType]\n",
    "                    correct = 1 if estimatedTagset in correctTagsetList else 0\n",
    "                    rowStr.append(estimatedTagset)\n",
    "                else:\n",
    "                    correct = 0\n",
    "                    rowStr.append('')\n",
    "                rowStr.append(correct)\n",
    "            rowStr.append(bestCandSet)\n",
    "            writer.writerow(rowStr)\n",
    "            \n",
    "                \n",
    "f = gdrive.CreateFile()\n",
    "f.SetContentFile(filename)\n",
    "f.Upload()\n",
    "f = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Transfer knowledge\n",
    "\n",
    "foundWordDict = OrderedDict()\n",
    "cnt = 0\n",
    "for (comb, candSet, candSetScore) in sorted(bestCandScoreDict.values(), key=itemgetter(2), reverse=True):\n",
    "    if cnt>coverageNum*0.2:\n",
    "        break\n",
    "    cnt +=1\n",
    "    sentence = comb.get_sentence()\n",
    "    for word in sentence:\n",
    "        meaningTagsetList = scoreDictDict[word].keys()\n",
    "        if len(meaningTagsetList)>1:\n",
    "            continue\n",
    "        meaningTagset = meaningTagsetList[0]\n",
    "        correctFlag = True\n",
    "        for meaningTag in meaningTagset.split():\n",
    "            pointTagset = candSet['point'].split('_')\n",
    "            if 'equip' in candSet.keys():\n",
    "                equipTagset = candSet['equip'].split('_')\n",
    "            else:\n",
    "                equipTagset = []\n",
    "            if meaningTag not in pointTagset+equipTagset:\n",
    "                correctFlag = False\n",
    "        if correctFlag:\n",
    "            foundWordDict[word] = meaningTagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Finished!!!\"\n",
    "from IPython.display import Audio\n",
    "sound_file = 'etc/fins_success.wav'\n",
    "Audio(url=sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combList = origCombListDict[143]\n",
    "for comb in combList:\n",
    "    print comb.get_cand_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '['\n",
    "for i in range(0,10):\n",
    "    print int(random.random() * 2334), ','\n",
    "    \n",
    "print ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluation!!!!!!!!!!!\n",
    "\n",
    "# Evaluate scoring\n",
    "sentenceIdxList = [\n",
    "1609 ,\n",
    "1317 ,\n",
    "418 ,\n",
    "321 ,\n",
    "305 ,\n",
    "283 ,\n",
    "2274 ,\n",
    "558 ,\n",
    "813 ,\n",
    "1289 ,\n",
    "]\n",
    "wordList = list()\n",
    "for idx in sentenceIdxList:\n",
    "    #print idx\n",
    "    sentence = origSentenceDict[idx]\n",
    "    wordList += sentence\n",
    "wordList = list(set(wordList))\n",
    "print wordList\n",
    "if buildingName == 'ebu3b':\n",
    "    evalWordMeaningDict = {'penthouse':'',\n",
    "                           'code':'',\n",
    "                           'feedback':'',\n",
    "                           'ah':'ahu',\n",
    "                           'setpoint': 'setpoint',\n",
    "                           'supflow': 'supply flow',\n",
    "                           'meter': 'meter',\n",
    "                           'vma': '',\n",
    "                           'actclg': 'effective cooling',\n",
    "                           'cmd': 'command',\n",
    "                           'bonner':'',\n",
    "                           'n': '',\n",
    "                           'occupied': 'occupied',\n",
    "                           'sav':'vav',\n",
    "                           'temperature':'temperature',\n",
    "                           'nae':'',\n",
    "                           'occ':'occupied',\n",
    "                           'clg':'cooling',\n",
    "                           'nd':'',\n",
    "                           'efs':'exhaust fan',  ####TODO Check\n",
    "                           'rd':'',\n",
    "                           'th':'',\n",
    "                           'rm':'room',\n",
    "                           'bypdmp':'bypass damper',\n",
    "                           'coomand':'command',\n",
    "                           'common':'',\n",
    "                           'ef':'exhaust fan',\n",
    "                           'bypass':'bypass',\n",
    "                           'fc':'',\n",
    "                           'stpt':'setpoint',\n",
    "                           'dx':'',\n",
    "                           'demand':'',\n",
    "                           'dmd':'',\n",
    "                           'fec':'',\n",
    "                           'a':'',\n",
    "                           'c':'',\n",
    "                           'actual':'',\n",
    "                           'flr':'floor',\n",
    "                           'unlock':'',\n",
    "                           'roof':'',\n",
    "                           't':'',\n",
    "                           'st':'setpoint',\n",
    "                           'sa':'supply air',\n",
    "                           'com':'',\n",
    "                           'vnd':'',\n",
    "                           'stby':'standby',\n",
    "                           'ebu':'',\n",
    "                           'on':'',\n",
    "                           'adj':'adjust',\n",
    "                           'dmpr':'damper',\n",
    "                           'pos':'position',\n",
    "                           'acthtgsp':'effective heating temperature setpoint',\n",
    "                           'actclgsp':'effective cooling temperature setpoint',\n",
    "                           'sup':'supply',\n",
    "                           'commonsp': 'setpoint',\n",
    "                           'dpr':'damper',\n",
    "                           \n",
    "    }\n",
    "elif buildingName=='bonner':\n",
    "    evalWordMeaningDict = {'penthouse':'', \n",
    "                            'code':'', \n",
    "                            'feedback':'', \n",
    "                            'ah':'ahu', \n",
    "                            'supflow':'supply air flow',\n",
    "                            'vma':'', \n",
    "                            'actclg':'effective cooling',\n",
    "                            'cmd':'command', \n",
    "                            'bonner':'', \n",
    "                            'n':'', \n",
    "                            'sav':'vav', \n",
    "                            'nae':'',\n",
    "                            'occ':'occupied', \n",
    "                            'clg':'cooling', \n",
    "                            'nd':'', \n",
    "                            'efs':'exhaust fan status', \n",
    "                            'rd':'', \n",
    "                            'th':'', \n",
    "                            'rm':'room', \n",
    "                            'bypdmp':'bypass damper', \n",
    "                            'coomand':'command',\n",
    "                           'common':'',\n",
    "                           'ef':'exhaust fan',\n",
    "                           'bypass':'bypass',\n",
    "                           'fc':'',\n",
    "                           'stpt':'setpoint',\n",
    "                           'dx':'',\n",
    "                           'demand':'',\n",
    "                           'dmd':'',\n",
    "                           'fec':'',\n",
    "                           'a':'',\n",
    "                           'c':'',\n",
    "                           'actual':'',\n",
    "                           'flr':'floor',\n",
    "                            'percent':'percent', \n",
    "                            'actual':'', \n",
    "                            'flr':'floor', \n",
    "                           'unlock':'',\n",
    "                           'roof':'',\n",
    "                           't':'', \n",
    "                           'st':'setpoint', \n",
    "                           'sa':'supply air', \n",
    "                           'com':'command'}\n",
    "elif 'ap_m':\n",
    "    evalWordMeaningDict = {'htg':'hetaing',\n",
    "                           'int':'integral', \n",
    "                           'it':'integral', \n",
    "                           'vp':'', \n",
    "                           'ap':'',\n",
    "                           'minimum':'min', \n",
    "                           'aclg':'effective cooling',\n",
    "                           'ahtg':'effective heating', \n",
    "                           'sathtg': 'supply air heating', \n",
    "                           'parameters':'', \n",
    "                           'nae':'', \n",
    "                           'occ':'occupied', \n",
    "                           'rm':'room', \n",
    "                           'wc':'warm cool', \n",
    "                           'tuning':'', 'stpt':'setpoint', 'trunk':'', 'vlv':'valve', 'a':'', 'actual':'effective', \n",
    "                           'ohmin':'occupied heating min', \n",
    "                           'adj':'adjust', 'cmd':'command', 'm':'', 'n':'', 'omin':'occupied min', \n",
    "                           's':'', 'time':'', 'sa':'supply air', 'adjustment':'adjust'\n",
    "                          }\n",
    "    \n",
    "for word in wordList:\n",
    "    if word in tagList:\n",
    "        evalWordMeaningDict[word] = word\n",
    "    \n",
    "evalScoreDictDict = dict()\n",
    "for word, meaning in evalWordMeaningDict.items():\n",
    "    evalScoreDictDict[word] = {meaning:1}\n",
    "evalMaxScoreDict = calc_max_score_dict(evalScoreDictDict)\n",
    "\n",
    "evalCombListDict = dict()\n",
    "for idx in sentenceIdxList:\n",
    "    sentence = origSentenceDict[idx]\n",
    "    combList = init_comb((sentence, subTagListDictList, \\\n",
    "                          evalScoreDictDict, 0))\n",
    "    evalCombListDict[idx] = combList\n",
    "    \n",
    "\n",
    "for i in range(0,10):\n",
    "    evalCombListDict = eval_weight((evalCombListDict, evalScoreDictDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreGetter = lambda x:x.get_score()\n",
    "for i, combList in evalCombListDict.items():\n",
    "    bestComb = sorted(combList, key=scoreGetter, reverse=True)[0]\n",
    "    print i, bestComb.get_sentence(), bestComb.get_cand_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combList = evalCombListDict[2274]\n",
    "for comb in sorted(combList, key=scoreGetter,reverse=True):\n",
    "    print comb.get_cand_set(), comb.get_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "assert(False)\n",
    "import google_drive\n",
    "reload(google_drive)\n",
    "from google_drive import gdrive"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
