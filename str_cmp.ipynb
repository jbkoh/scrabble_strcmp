{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n"
     ]
    }
   ],
   "source": [
    "import jellyfish as jf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import building_tokenizer as toker\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from ontolcs import ontolcs\n",
    "from operator import itemgetter\n",
    "import csv\n",
    "import re\n",
    "import operator\n",
    "import math\n",
    "from pyjarowinkler import distance\n",
    "from stringscore import liquidmetal\n",
    "from itertools import chain\n",
    "import os\n",
    "import get_schema2\n",
    "reload(get_schema2)\n",
    "#from get_schema2 import tagList, tagsetList\n",
    "from brick_parser import tagList, tagsetList, equalDict\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "f = lambda x:x.replace('_', ' ')\n",
    "tagsetList = map(f, tagsetList)\n",
    "termList = list(set(tagList+tagsetList))\n",
    "\n",
    "from simple_map_reduce import SimpleMapReduce\n",
    "import multiprocessing\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "buildingName = 'ghc'\n",
    "ucsdBuildings = ['ebu3b', 'ap_m', 'bsb', 'bonner']\n",
    "\n",
    "if buildingName in ucsdBuildings:\n",
    "\n",
    "    naeDict = dict()\n",
    "    naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "    naeDict['ap_m'] = ['514', '513','604']\n",
    "    naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "    naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "    naeList = naeDict[buildingName]\n",
    "\n",
    "    labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "    with open(labeledFile, 'rb') as fp:\n",
    "        #truthDF = pd.read_excel(fp)\n",
    "        truthDF = pd.DataFrame.from_csv(fp)\n",
    "        #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "    wordFeatFile = 'data/wordfeat_'+buildingName+'.pkl'\n",
    "\n",
    "    tokenType = 'Alphanumeric'\n",
    "    #tokenType = 'NumAsSingleWord'\n",
    "    #tokenType = 'NoNumber'\n",
    "    tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "    bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "    for val in Counter(unitMap.keys()).values():\n",
    "        if val>1:\n",
    "            \"Unit map file ERROR\"\n",
    "            assert(False)\n",
    "\n",
    "    tokenType = 'NoNumber'\n",
    "    trueDF = pd.DataFrame.from_csv('metadata/'+buildingName+'_sensor_types_location.csv')\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, _, wordList = \\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \\\n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "else:\n",
    "    filename = 'metadata/'+buildingName+'_sensor_types_location.csv'\n",
    "    #   filename = 'metadata/%s_sensor_types_location.csv'%buildingName\n",
    "    df = pd.read_csv(filename)\n",
    "    sentenceList = list()\n",
    "    \n",
    "    tokenType = 'NoNumber'\n",
    "    for raw in df['bas_raw'].tolist():\n",
    "        sentenceList.append(toker.tokenize(tokenType, raw))\n",
    "    adder = lambda x,y:x+y\n",
    "    wordList = list(set(reduce(adder,sentenceList,[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ported from Quicksilver's scoreForAbbreviation algorithm\n",
    "# https://github.com/quicksilver/Quicksilver/\n",
    "\n",
    "SCORE_NO_MATCH = 0.0\n",
    "SCORE_MATCH = 1.0\n",
    "SCORE_TRAILING = 0.9\n",
    "SCORE_BUFFER = 0.15\n",
    "WHITESPACE_CHARACTERS = ' \\t'\n",
    "\n",
    "\n",
    "def quicksilver_score2(string, abbrev):\n",
    "    abbrev = abbrev.lower()\n",
    "    abbrev_len = len(abbrev)\n",
    "    string_len = len(string)\n",
    "\n",
    "    # deduct some points for all remaining letters\n",
    "    if abbrev_len == 0:\n",
    "        return SCORE_TRAILING\n",
    "    if abbrev_len > string_len:\n",
    "        return SCORE_NO_MATCH\n",
    "\n",
    "    # Search for steadily smaller portions of the abbreviation\n",
    "    for i in range(abbrev_len, 0, -1):\n",
    "        try:\n",
    "            index = string.lower().index(abbrev[:i])\n",
    "        except ValueError:\n",
    "            continue  # Not found\n",
    "\n",
    "        if index + abbrev_len > string_len:\n",
    "            continue\n",
    "\n",
    "        next_string = string[index + i:]\n",
    "        next_abbrev = abbrev[i:]\n",
    "\n",
    "        # Search what is left of the string with the rest of the abbreviation\n",
    "        remaining_score = quicksilver_score2(next_string, next_abbrev)\n",
    "\n",
    "        if remaining_score > 0:\n",
    "            result_score = index + i\n",
    "\n",
    "            # ignore skipped characters if is first letter of a word\n",
    "            if index > 0:  # if some letters were skipped\n",
    "                if string[index - 1] in WHITESPACE_CHARACTERS:\n",
    "                    for j in range(index - 1):\n",
    "                        c = string[j]\n",
    "                        result_score -= SCORE_MATCH \\\n",
    "                                        if c in WHITESPACE_CHARACTERS \\\n",
    "                                        else SCORE_BUFFER\n",
    "                elif 'A' <= string[index] <= 'Z':\n",
    "                    for j in range(index):\n",
    "                        c = string[j]\n",
    "                        result_score -= SCORE_MATCH \\\n",
    "                                        if 'A' <= c <= 'Z' \\\n",
    "                                        else SCORE_BUFFER\n",
    "                else:\n",
    "                    result_score -= index\n",
    "\n",
    "            result_score += remaining_score * len(next_string)\n",
    "            result_score /= string_len\n",
    "            return result_score\n",
    "\n",
    "    return SCORE_NO_MATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Quicksilver: String to abbreviation ranking algorithm\n",
    "def quicksilver_score(s, a, offset=0):\n",
    "    s = s.lower()\n",
    "    a = a.lower()\n",
    "    if len(a)==0:\n",
    "        return 0.9\n",
    "    if len(a)>len(s):\n",
    "        return 0\n",
    "    \n",
    "    for i in reversed(range(1,len(a)+1)):\n",
    "        subA = a[0:i]\n",
    "        if subA in s:\n",
    "            index = s.index(subA)\n",
    "        else:\n",
    "            continue\n",
    "        if index<0:\n",
    "            continue\n",
    "        if index+len(a) > len(s) + offset:\n",
    "            continue\n",
    "            \n",
    "        nextS = s[index+len(subA):]\n",
    "        nextA = None\n",
    "        if i>=len(a):\n",
    "            nextA = ''\n",
    "        else:\n",
    "            nextA = a[i:]\n",
    "        remainingScore = quicksilver_score(nextS, nextA, offset+index)\n",
    "        if remainingScore>0:\n",
    "            score = len(s) - len(nextS)\n",
    "            if index is not 0:\n",
    "                #c = ord(s[index-1:][0])\n",
    "                #if c==32 or c==9:\n",
    "                if s[index-1]==' ':\n",
    "                    for j in reversed(range(0,index-2)):\n",
    "                        #c = ord(s[j])\n",
    "                        #score -= (1 if c==32 or c==9 else 0.15)\n",
    "                        score -= (1 if s[j]==' ' else 0.15)\n",
    "                else:\n",
    "                    score -= index\n",
    "            score += remainingScore * len(nextS)\n",
    "            score /= len(s)\n",
    "            return score\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913076923077\n",
      "-------\n",
      "0.925\n",
      "-------\n",
      "0.635483870968\n",
      "-------\n",
      "0.852173913043\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "#Quicksilver: String to abbreviation ranking algorithm\n",
    "def modified_quicksilver_score(s, a, offset=0, aPos=0):\n",
    "    s = s.lower()\n",
    "    a = a.lower()\n",
    "    if len(a)==0:\n",
    "        return 0.9 - 0.01*sum([1 if ' '==oneS else 0 for oneS in s])\n",
    "    if len(a)>len(s):\n",
    "        return 0\n",
    "    \n",
    "    for i in reversed(range(1,len(a)+1)):\n",
    "        subA = a[0:i]\n",
    "        if subA in s:\n",
    "            index = s.index(subA)\n",
    "        else:\n",
    "            continue\n",
    "        if index<0:\n",
    "            continue\n",
    "        if index+len(a) > len(s) + offset:\n",
    "            continue\n",
    "            \n",
    "        nextS = s[index+len(subA):]\n",
    "        nextA = None\n",
    "        if i>=len(a):\n",
    "            nextA = ''\n",
    "        else:\n",
    "            nextA = a[i:]\n",
    "        remainingScore = modified_quicksilver_score(nextS, nextA, offset+index, aPos+i)\n",
    "        \n",
    "        \n",
    "        if remainingScore>0:\n",
    "            currS = s[:index+len(subA)]\n",
    "            if ' ' in currS:\n",
    "                spaceIndices = [i for i,c in enumerate(currS) if c==' ']\n",
    "                if len(spaceIndices)>=1 and aPos==0:# and False:\n",
    "                    score = len(s[spaceIndices[-1]:]) - len(nextS) - 0.01*(len(spaceIndices)-1)\n",
    "                else:\n",
    "                    score = len(s) - len(nextS)\n",
    "            else:\n",
    "                score = len(s) - len(nextS)\n",
    "            \n",
    "            #score = len(s) - len(nextS)\n",
    "            \n",
    "            #print s, '/', a, '/', currS, '/', aPos\n",
    "            #print score\n",
    "            \n",
    "            if index is not 0:\n",
    "                #c = ord(s[index-1:][0])\n",
    "                #if c==32 or c==9:\n",
    "                if s[index-1]==' ':\n",
    "                    for j in reversed(range(0,index-2)):\n",
    "                        #c = ord(s[j])\n",
    "                        #score -= (1 if c==32 or c==9 else 0.15)\n",
    "                        if s[j]==' ':\n",
    "                            score -= 1  \n",
    "                        else:\n",
    "                            score -= 0.15\n",
    "                else:\n",
    "                    score -= index\n",
    "            score += remainingScore * len(nextS)\n",
    "            score /= len(s)\n",
    "            return score\n",
    "    return 0\n",
    "\n",
    "#print modified_quicksilver_score('differential pressure', 'dp')\n",
    "#print modified_quicksilver_score('dewpoint setpoint', 'dp')\n",
    "print modified_quicksilver_score('chilled water pump command', 'chwp')\n",
    "print '-------'\n",
    "print modified_quicksilver_score('chilled water pump', 'chwp')\n",
    "print '-------'\n",
    "print modified_quicksilver_score('warmest zone temperature sensor', 'zn')\n",
    "print '-------'\n",
    "print modified_quicksilver_score('zone temperature sensor', 'zn')\n",
    "print '-------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Longest common substring\n",
    "def lcs_len(X, Y):\n",
    "    m = len(X)\n",
    "    n = len(Y)\n",
    "    # An (m+1) times (n+1) matrix\n",
    "    C = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(1, m+1):\n",
    "        for j in range(1, n+1):\n",
    "            if X[i-1] == Y[j-1]: \n",
    "                C[i][j] = C[i-1][j-1] + 1\n",
    "            else:\n",
    "                C[i][j] = max(C[i][j-1], C[i-1][j])\n",
    "    lenList = [subC[-1] for subC in C]\n",
    "    return max(lenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if buildingName in ucsdBuildings:\n",
    "    unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "\n",
    "    tokenType = 'NoNumber'\n",
    "    sensorDF, nameList, jcinameList, descList, unitList, _, wordList = \\\n",
    "    toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \n",
    "                             validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n",
    "\n",
    "    #wordList: List of all words after tokenizing metadata of a building\n",
    "    #alphaWordList: List of alphabetical words only\n",
    "    alphaWordList = list()\n",
    "    for word in wordList:\n",
    "        alphaWordList += re.findall('[a-zA-Z]+', word)\n",
    "    addFunc = lambda a,b:a+b\n",
    "    wordList = list(set(alphaWordList+reduce(addFunc,[unit.split() for unit in unitMap['word'].tolist() if type(unit)==str])))\n",
    "    try:\n",
    "        wordList.remove(np.nan)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    print len(wordList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_metric_schema(metric, wordList, termList):\n",
    "    calcDistPerWord = lambda s:dict([(term,apply(metric,[s,term])) for term in termList])\n",
    "    constructTuple = lambda word:tuple((word,apply(calcDistPerWord,[word])))\n",
    "    distDictDict = dict(map(constructTuple,wordList))\n",
    "    return distDictDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def map_func(metric, word, termList):\n",
    "    calcDistPerWord = lambda s:dict([(term,apply(metric,[s,term])) for term in termList])\n",
    "    #constructTuple = lambda word:tuple((word,apply(calcDistPerWord,[word])))\n",
    "    return tuple((word, apply(calcDistPerWord,[word])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metricDict = OrderedDict()\n",
    "metricDict['ontolcs'] = ontolcs\n",
    "#jaroLambda = lambda a,b: distance.get_jaro_distance(unicode(a), unicode(b), scaling=0.1)\n",
    "jaroLambda = lambda a,b: jf.jaro_winkler(unicode(a), unicode(b))\n",
    "metricDict['jaro'] = jaroLambda\n",
    "#quicksilverLambda = lambda a,b: liquidmetal.score(unicode(b), unicode(a))\n",
    "quicksilverLambda = lambda a,b: modified_quicksilver_score(unicode(b), unicode(a))\n",
    "metricDict['quicksilver'] = quicksilverLambda\n",
    "#matchRatingLambda = lambda a,b: distance.get_jaro_distance(jf.match_rating_codex(unicode(b)), unicode(a))\n",
    "#metricDict['match_rating'] = matchRatingLambda\n",
    "\n",
    "#lcsLambda = lambda a,b:lcs_len(a,b)/np.sqrt(float(len(a))*len(b))\n",
    "#metricDict['lcs'] = lcsLambda\n",
    "\n",
    "editLambda = lambda a,b:1-jf.levenshtein_distance(unicode(a), unicode(b))/float(max(len(a),len(b)))\n",
    "metricDict['edit'] = editLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalTagsetDistDict = dict()\n",
    "totalTagTagsetDistDict = dict()\n",
    "totalTagDistDict = dict()\n",
    "\n",
    "mpFlag = False\n",
    "if mpFlag:\n",
    "    print datetime.now()\n",
    "    totalTagsetDistDict = dict()\n",
    "    totalTagDistDict = dict()\n",
    "    workerNum = 4\n",
    "    pool = multiprocessing.Pool(workerNum)\n",
    "    for key, metric in metricDict.items():\n",
    "        #localMapFunc = lambda x:map_func(deepcopy(metric),x, deepcopy(tagsetList))\n",
    "        print key, datetime.now()\n",
    "\n",
    "        #Tagset Dist\n",
    "        def localTagsetMapFunc(x):\n",
    "            return map_func(metric,x, tagsetList)\n",
    "        mapResult = pool.map(localTagsetMapFunc, wordList, chunksize=len(wordList)/workerNum+1)\n",
    "        totalTagsetDistDict[metric] = dict(mapResult)\n",
    "    print \"Finished: \", datetime.now()\n",
    "else:\n",
    "    totalDistDict = dict()\n",
    "    for key, metric in metricDict.items():\n",
    "        #totalTagsetDistDict[key] = apply_metric_schema(metric, wordList, tagsetList)\n",
    "        #totalTagDistDict[key] = apply_metric_schema(metric, wordList, tagList)\n",
    "        totalTagTagsetDistDict[key] = apply_metric_schema(metric, wordList, list(set(tagList+tagsetList)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_dist_dict(totalDistDict):\n",
    "    #totalDistDict = deepcopy(totalTagsetDistDict)\n",
    "    for metric, tagsetDistDict in totalDistDict.items():\n",
    "        newTagsetDistDict = dict()\n",
    "        for i, (word, distDict) in enumerate(tagsetDistDict.items()):\n",
    "            maxVal = float(max(distDict.values()))\n",
    "            minVal = float(min(distDict.values()))\n",
    "            distDict = OrderedDict(distDict)\n",
    "            if maxVal==0:\n",
    "                newTagsetDistDict[word] = dict((key,0.5) for key in distDict.keys())\n",
    "            else:\n",
    "                normalize = lambda datum: (datum-minVal)/maxVal\n",
    "                values = map(normalize,distDict.values())\n",
    "                newTagsetDistDict[word] = dict((key,val) for key, val in \\\n",
    "                                               zip(distDict.keys(), values))\n",
    "        totalDistDict[metric] = newTagsetDistDict\n",
    "    return totalDistDict\n",
    "\n",
    "#totalTagsetDistDict = normalize_dist_dict(totalTagsetDistDict)\n",
    "#totalTagDistDict = normalize_dist_dict(totalTagDistDict)\n",
    "totalTagTagsetDistDict = normalize_dist_dict(totalTagTagsetDistDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagDF = pd.DataFrame(index=wordList)\n",
    "tagsetDF = pd.DataFrame(index=wordList)\n",
    "def distdict_df(totalDistDict, df):\n",
    "    for metricName in totalDistDict.keys():\n",
    "        valList = list()\n",
    "        for word in df.index.tolist():\n",
    "            maxVal = max(totalDistDict[metricName][word].values())\n",
    "            maxIdx = totalDistDict[metricName][word].values().index(maxVal)\n",
    "            maxKey = totalDistDict[metricName][word].keys()[maxIdx]\n",
    "            valList.append(maxKey)\n",
    "        df[metricName] = valList\n",
    "    return df\n",
    "\n",
    "#tagsetDF = distdict_df(totalTagsetDistDict, tagsetDF)\n",
    "#tagDF = distdict_df(totalTagDistDict, tagDF)\n",
    "tagtagsetDF = distdict_df(totalTagTagsetDistDict, tagDF)\n",
    "\n",
    "#tagsetDF.to_excel('result/tagset_comp_result.xlsx')\n",
    "#tagDF.to_excel('result/tag_comp_result.xlsx')\n",
    "tagtagsetDF.to_excel('result/tagtagset_comp_result.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totalFeatDict = dict()\n",
    "metricNum = len(metricDict.keys())\n",
    "zeroFeat = [0 for i in range(metricNum)]\n",
    "for word in wordList:\n",
    "    featDict = OrderedDict()\n",
    "    for term in termList:\n",
    "        featDict[term] = deepcopy(zeroFeat)\n",
    "    totalFeatDict[word] = featDict\n",
    "for metricIdx, (metricName, totalDistDict) in enumerate(totalTagTagsetDistDict.items()):\n",
    "    for word, distDict in totalDistDict.items():\n",
    "        for term, dist in distDict.items():\n",
    "            totalFeatDict[word][term][metricIdx] = dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scoreDictDict = dict()\n",
    "\n",
    "notUseSynonymList = equalDict.keys()\n",
    "useSynonymList = ['vav','ahu', 'vfd', 'crac']\n",
    "pickithElements = lambda xList,i:[x[i] for x in xList]\n",
    "\n",
    "scoreSummaryDict = dict()\n",
    "\n",
    "oneDimExtract = lambda x:x[i]\n",
    "\n",
    "for word in wordList:\n",
    "    #word = 'chwdp'\n",
    "    pcaN = 2\n",
    "    pca = PCA(n_components=pcaN)\n",
    "    pca.fit(totalFeatDict[word].values())\n",
    "    for j, component in enumerate(pca.components_):\n",
    "        if np.inner(component, np.ones(len(metricDict)))<0:\n",
    "            pca.components_[j] = -component\n",
    "    values = pca.transform(totalFeatDict[word].values())\n",
    "    '''\n",
    "    scoreDict = dict((k,v) for k,v in zip(totalFeatDict[word].keys(), map(sum,values)))\n",
    "    orderedScoreDict = OrderedDict(sorted(scoreDict.items(), key=itemgetter(1), reverse=True)[0:10])\n",
    "    '''\n",
    "    '''\n",
    "    subOrderedScoreDictList = list()\n",
    "    for i in range(0, pcaN):\n",
    "        oneDimExtract = lambda x:x[i]\n",
    "        scoreDict = dict((k,v) for k,v in zip(totalFeatDict[word].keys(), map(oneDimExtract,values)))\n",
    "        subOrderedScoreDictList.append(OrderedDict(sorted(scoreDict.items(), key=itemgetter(1), reverse=True)[0:5]))\n",
    "    \n",
    "    orderedScoreDict = dict()\n",
    "    for subOrderedScoreDict in subOrderedScoreDictList:\n",
    "        orderedScoreDict = dict(orderedScoreDict.items()+subOrderedScoreDict.items())\n",
    "    '''\n",
    "    orderedScoreDict = defaultdict(float)\n",
    "    featDict = OrderedDict(totalFeatDict[word])\n",
    "    for i in range(0,len(featDict.values()[0])):\n",
    "        valueList = pickithElements(featDict.values(),i)\n",
    "        pickedValueList = sorted(valueList, reverse=True)[0:3]\n",
    "        addedIndices = list()\n",
    "        for pickedValue in pickedValueList:\n",
    "            pickedIndices = [idx for idx, val in enumerate(valueList) if val==pickedValue]\n",
    "            for pickedIdx in pickedIndices[0:3]:\n",
    "                if pickedIdx not in addedIndices:\n",
    "                    addedIndices.append(pickedIdx)\n",
    "                    orderedScoreDict[featDict.keys()[pickedIdx]] += pickedValue\n",
    "    orderedScoreDict = dict(orderedScoreDict)\n",
    "    \n",
    "    scoreDictDict[word] = orderedScoreDict\n",
    "    scoreSummaryDict[(word,orderedScoreDict.keys()[0])] = orderedScoreDict.values()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== edit\n",
      "1.0 2\n",
      "di\n",
      "dc\n",
      "1.0 2\n",
      "0.666666666667 2\n",
      "damper\n",
      "dew\n",
      "0.666666666667 2\n",
      "0.5 9\n",
      "code\n",
      "mode\n",
      "duct\n",
      "dewpoint\n",
      "open\n",
      "=== ontolcs\n",
      "1.0 2\n",
      "di\n",
      "dc\n",
      "1.0 2\n",
      "0.960698689956 1\n",
      "pv\n",
      "0.876232064878 1\n",
      "dew\n",
      "0.829070492826 4\n",
      "led\n",
      "pre\n",
      "pir\n",
      "vfd\n",
      "=== jaro\n",
      "1.0 1\n",
      "damper\n",
      "0.964285714286 1\n",
      "dewpoint\n",
      "0.918367346939 1\n",
      "damper command\n",
      "0.914285714286 2\n",
      "dewpoint sensor\n",
      "damper position\n",
      "0.914285714286 2\n",
      "=== quicksilver\n",
      "1.77150924593 1\n",
      "dew point setpoint\n",
      "1.75289083633 1\n",
      "differential pressure\n",
      "1.74871474445 1\n",
      "differential pressure sensor\n",
      "1.74861034215 1\n",
      "differential pressure setpoint\n",
      "1.74182419286 1\n",
      "differential pressure integral time\n"
     ]
    }
   ],
   "source": [
    "for metricName, distDict in totalTagTagsetDistDict.items():\n",
    "    print '===', metricName\n",
    "    distDict = distDict['dp']\n",
    "    values = sorted(distDict.values(), reverse=True)[0:5]\n",
    "    printedIndices = list()\n",
    "    for val in values:\n",
    "        indices = [idx for idx,value in enumerate(distDict.values()) if value==val]\n",
    "        print val, len(indices)\n",
    "        for idx in indices[0:5]:\n",
    "            if not (idx in printedIndices):\n",
    "                print distDict.keys()[idx]\n",
    "                printedIndices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.876666666667\n",
      "0.895238095238\n",
      "0.894791666667\n",
      "0.888095238095\n"
     ]
    }
   ],
   "source": [
    "print quicksilverLambda('dp', 'differential pressure proportional band setpoint')\n",
    "print quicksilverLambda('dp', 'differential pressure')\n",
    "\n",
    "print quicksilver_score2('differential pressure proportional band setpoint', 'dp')\n",
    "print quicksilver_score2('differential pressure', 'dp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/'+buildingName+'_str_score_dict.pkl', 'wb') as fp:\n",
    "    pickle.dump(scoreDictDict, fp)\n",
    "    \n",
    "tuplelist2list = lambda x:list(x)\n",
    "addFunc = lambda x,y:x+y\n",
    "with open('result/'+buildingName+'_str_comp_score.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    writer.writerow(['word in metadata', '[tagset, score]'])\n",
    "    for key, scoreDict in scoreDictDict.items():\n",
    "        writer.writerow([key]+reduce(addFunc,map(list,scoreDict.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('result/'+buildingName+'_wordmapping.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for key, score in scoreSummaryDict.items():\n",
    "        writer.writerow([key[0], key[1],score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alm alarm\n",
      "deceleration deceleration\n",
      "stndby standby\n",
      "freq frequency\n",
      "chwdp chilled water differential pressure sensor\n"
     ]
    }
   ],
   "source": [
    "vectorList = list()\n",
    "pairList = list()\n",
    "for word, featDict in totalFeatDict.items():\n",
    "    for tagTagset, vector in featDict.items():\n",
    "        vectorList.append(vector)\n",
    "        pairList.append((word,tagTagset))\n",
    "learningSampleSet = [\n",
    "('alm', 'alarm'),\n",
    "('deceleration', 'deceleration'),\n",
    "('stndby', 'standby'),\n",
    "('freq', 'frequency'),\n",
    "('chwdp', 'chilled water differential pressure sensor'),\n",
    "]\n",
    "learningVectorList = list()\n",
    "learningLabelList = list()\n",
    "origIndexList = list()\n",
    "learningPairList = list()\n",
    "for word, tagTagset in learningSampleSet:\n",
    "    print word, tagTagset\n",
    "    for i, pair in enumerate(pairList):\n",
    "        if pair[0]==word:\n",
    "            learningVectorList.append(vectorList[i])\n",
    "            origIndexList.append(i)\n",
    "            if pair[1]==tagTagset:\n",
    "                learningLabelList.append(1)\n",
    "            else:\n",
    "                learningLabelList.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbkoh/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-65ca0af44ddd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#learner = BernoulliNB()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m#learner = LogisticRegression()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearningVectorList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearningLabelList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mpredLabelList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorList\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jbkoh/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 405\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jbkoh/anaconda2/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         X, y = check_X_y(X, y, accept_sparse=accept_sparse, dtype=dtype,\n\u001b[1;32m--> 111\u001b[1;33m                          y_numeric=is_regressor(self))\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jbkoh/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    508\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[0;32m    509\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m/home/jbkoh/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 415\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "learner = AdaBoostClassifier()\n",
    "#learner = RandomForestClassifier()\n",
    "#learner = DecisionTreeClassifier()\n",
    "#learner = BernoulliNB()\n",
    "#learner = LogisticRegression()\n",
    "learner.fit(learningVectorList, learningLabelList)\n",
    "\n",
    "predLabelList = learner.predict(vectorList)\n",
    "predScoreList = learner.predict_proba(vectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('result/temporary_word_classification_result.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for i, (pair, label) in enumerate(zip(pairList,predLabelList)):\n",
    "        writer.writerow([pair[0], pair[1], label])\n",
    "\n",
    "with open('result/temporary_word_classification_score_result.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for i, (pair, score) in enumerate(zip(pairList,predScoreList)):\n",
    "        writer.writerow([pair[0], pair[1], \"%.3f\" % score[1]])\n",
    "        \n",
    "with open('result/temporary_word_classification_result_label1.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for i, (pair, label) in enumerate(zip(pairList,predLabelList)):\n",
    "        if label==1:\n",
    "            writer.writerow([pair[0], pair[1], label])\n",
    "\n",
    "foundList = list()\n",
    "with open('result/temporary_word_classification_result_label1_nonredundant.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    \n",
    "    for i, (pair, label) in enumerate(zip(pairList,predLabelList)):\n",
    "        if pair[0] in foundList:\n",
    "            continue\n",
    "        if label==1:\n",
    "            writer.writerow([pair[0], pair[1], label])\n",
    "            foundList.append(pair[0])\n",
    "notfoundList = list()\n",
    "with open('result/temporary_word_classification_result_label1_notfound.csv', 'wb') as fp:\n",
    "    writer = csv.writer(fp)\n",
    "    for i, (pair, label) in enumerate(zip(pairList,predLabelList)):\n",
    "        if pair[0] not in foundList:\n",
    "            writer.writerow([pair[0], pair[1], predScoreList[i][1]])\n",
    "            if pair[0] not in notfoundList:\n",
    "                notfoundList.append(pair[0])\n",
    "print \"Found: \", len(foundList)\n",
    "print \"Not Found: \", len(notfoundList)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
