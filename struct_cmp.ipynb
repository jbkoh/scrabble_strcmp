{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n",
      "Init Graph\n",
      "Load Brick.ttl\n",
      "Load BrickFrame.ttl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jbkoh/anaconda2/lib/python2.7/site-packages/matplotlib/__init__.py:1350: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#essential libraries\n",
    "\n",
    "#from google_drive import gdrive\n",
    "\n",
    "from matplotlib.pyplot import show\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import *\n",
    "import operator\n",
    "import matplotlib\n",
    "reload(matplotlib)\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "import shelve\n",
    "import re\n",
    "from collections import Counter, defaultdict, OrderedDict, deque\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import metrics\n",
    "import scipy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import csv\n",
    "import sys\n",
    "import math\n",
    "from copy import deepcopy, copy\n",
    "import random\n",
    "import rosetta\n",
    "from datetime import datetime, timedelta\n",
    "from operator import itemgetter\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import rdflib\n",
    "from rdflib.namespace import OWL, RDF, RDFS\n",
    "from rdflib import URIRef\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import OneClassSVM, SVC\n",
    "from sklearn.mixture import GMM\n",
    "from sklearn.mixture import DPGMM\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "from scipy import spatial\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import metrics\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import scipy.cluster.hierarchy as hier\n",
    "\n",
    "from scipy.spatial.distance import cosine as cosine_similarity\n",
    "\n",
    "from divergence import gau_js as js_divergence\n",
    "import building_tokenizer as toker\n",
    "import brick_parser\n",
    "reload(brick_parser)\n",
    "from brick_parser import tagList, tagsetList, equipTagsetList, pointTagsetList, locationTagsetList,\\\n",
    "equalDict, pointTagList, equipTagList, locationTagList, equipPointDict\n",
    "subTagListDict = dict([('point', pointTagList),\n",
    "                          ('equip', equipTagList),\n",
    "                          ('location', locationTagList)\n",
    "                         ])\n",
    "subTagsetListDict = dict([('point', pointTagsetList),\n",
    "                          ('equip', equipTagsetList),\n",
    "                          ('location', locationTagsetList)\n",
    "                         ])\n",
    "\n",
    "debugFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pick_dict_with_best_n_from_dict(srcDict, n):\n",
    "    idxCnt = 0\n",
    "    sortedValueList = sorted(srcDict.values(), reverse=True)[0:n]\n",
    "    chosenItemList = list()\n",
    "    \n",
    "    for score in sortedValueList:\n",
    "        for key, val in srcDict.items():\n",
    "            if idxCnt>=n:\n",
    "                break\n",
    "            if val==score:\n",
    "                chosenItemList.append((key,val))\n",
    "                idxCnt += 1\n",
    "        if idxCnt>=n:\n",
    "                break\n",
    "    return OrderedDict(chosenItemList)\n",
    "\n",
    "def save_fig(fig, name, dpi=400):\n",
    "\tpp = PdfPages(name)\n",
    "\tpp.savefig(fig, bbox_inches='tight', pad_inches=0, dpi=dpi)\n",
    "\tpp.close()\n",
    "\n",
    "buildingName = 'ebu3b'\n",
    "\n",
    "naeDict = dict()\n",
    "naeDict['bonner'] = [\"607\", \"608\", \"609\", \"557\", \"610\"]\n",
    "naeDict['ap_m'] = ['514', '513','604']\n",
    "naeDict['bsb'] = ['519', '568', '567', '566', '564', '565']\n",
    "naeDict['ebu3b'] = [\"505\", \"506\"]\n",
    "naeList = naeDict[buildingName]\n",
    "\n",
    "labeledFile = 'metadata/' + buildingName + '_sensor_types_location.csv'\n",
    "with open(labeledFile, 'rb') as fp:\n",
    "    #truthDF = pd.read_excel(fp)\n",
    "    truthDF = pd.DataFrame.from_csv(fp)\n",
    "    #truthDF = truthDF.set_index(keys='Unique Identifier')\n",
    "\n",
    "wordFeatFile = 'data/wordfeat_'+buildingName+'.pkl'\n",
    "\n",
    "tokenTypeList = ['NoNumber', 'Alphanumeric', 'AlphaAndNum', 'NumAsSingleWord']\n",
    "\n",
    "bacnetTypeMapDF = pd.DataFrame.from_csv('metadata/bacnettype_mapping.csv')\n",
    "unitMap = pd.read_csv('metadata/unit_mapping.csv').set_index('unit')\n",
    "for val in Counter(unitMap.keys()).values():\n",
    "    if val>1:\n",
    "        \"Unit map file ERROR\"\n",
    "        assert(False)\n",
    "        \n",
    "tokenType = 'NoNumber'\n",
    "trueDF = pd.DataFrame.from_csv('metadata/'+buildingName+'_sensor_types_location.csv')\n",
    "sensorDF, nameList, jcinameList, descList, unitList, wordList = \\\n",
    "toker.structure_metadata(buildingName=buildingName, tokenType=tokenType, \\\n",
    "                         validSrcidList=trueDF.index.tolist(), withDotFlag=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'description', u'jci_name', u'name', u'type', u'type_string', u'unit'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensorDF.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoderDict = {'A':0, '1':1, '/':2, ' ':3, '.':4}\n",
    "underbarAdder = lambda x,y:str(x) + '_' + str(y)\n",
    "listMerger = lambda x:reduce(underbarAdder,x, '')\n",
    "\n",
    "structListDict = dict()\n",
    "structStrListDict = dict()\n",
    "\n",
    "for colName, col in sensorDF[['description', u'jci_name', u'name']].iteritems():\n",
    "    structList = list()\n",
    "    for srcid, sentence in col.iteritems():\n",
    "        newStr = []\n",
    "        for c in sentence:\n",
    "            if re.match('[a-zA-Z]', c):\n",
    "                newStr.append(0)\n",
    "            elif re.match('\\d+', c):\n",
    "                newStr.append(1)\n",
    "            else:\n",
    "                if c in encoderDict.keys():\n",
    "                    newStr.append(encoderDict[c])\n",
    "                else:\n",
    "                    encoderDict[c] = max(encoderDict.values())+1\n",
    "                    newStr.append(encoderDict[c])\n",
    "        removeIdx = [i+1 for i in range(0,len(newStr)-1) if newStr[i]==newStr[i+1]]\n",
    "        removeIdx.reverse()\n",
    "        for idx in removeIdx:\n",
    "            #newStr = newStr[:idx] + newStr[idx+1:]\n",
    "            del(newStr[idx])\n",
    "        structList.append(newStr)\n",
    "    structListDict[colName] = structList\n",
    "    structStrListDict[colName] = map(listMerger, structList)\n",
    "for colName, structStrList in structStrListDict.items():\n",
    "    sensorDF[colName+'_struct'] = structStrList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source_id\n",
      "506_0_3000023         NAE 06 N2 1 VMA102 ZN T\n",
      "506_0_3000028     NAE 06 N2 1 VMA102 DMPR POS\n",
      "506_0_3000029     NAE 06 N2 1 VMA102 SUP FLOW\n",
      "506_0_3000044         NAE 06 N2 1 VMA127 ZN T\n",
      "506_0_3000049     NAE 06 N2 1 VMA127 DMPR POS\n",
      "506_0_3000050     NAE 06 N2 1 VMA127 SUP FLOW\n",
      "506_0_3000064         NAE 06 N2 1 VMA128 ZN T\n",
      "506_0_3000069     NAE 06 N2 1 VMA128 DMPR POS\n",
      "506_0_3000070     NAE 06 N2 1 VMA128 SUP FLOW\n",
      "506_0_3000084         NAE 06 N2 1 VMA129 ZN T\n",
      "506_0_3000089     NAE 06 N2 1 VMA129 DMPR POS\n",
      "506_0_3000090     NAE 06 N2 1 VMA129 SUP FLOW\n",
      "506_0_3000104         NAE 06 N2 1 VMA130 ZN T\n",
      "506_0_3000109     NAE 06 N2 1 VMA130 DMPR POS\n",
      "506_0_3000110     NAE 06 N2 1 VMA130 SUP FLOW\n",
      "506_0_3000124         NAE 06 N2 1 VMA131 ZN T\n",
      "506_0_3000129     NAE 06 N2 1 VMA131 DMPR POS\n",
      "506_0_3000130     NAE 06 N2 1 VMA131 SUP FLOW\n",
      "506_0_3000144         NAE 06 N2 1 VMA132 ZN T\n",
      "506_0_3000149     NAE 06 N2 1 VMA132 DMPR POS\n",
      "506_0_3000150     NAE 06 N2 1 VMA132 SUP FLOW\n",
      "506_0_3000164         NAE 06 N2 1 VMA133 ZN T\n",
      "506_0_3000169     NAE 06 N2 1 VMA133 DMPR POS\n",
      "506_0_3000170     NAE 06 N2 1 VMA133 SUP FLOW\n",
      "506_0_3000184         NAE 06 N2 1 VMA134 ZN T\n",
      "506_0_3000189     NAE 06 N2 1 VMA134 DMPR POS\n",
      "506_0_3000190     NAE 06 N2 1 VMA134 SUP FLOW\n",
      "506_0_3000204         NAE 06 N2 1 VMA135 ZN T\n",
      "506_0_3000209     NAE 06 N2 1 VMA135 DMPR POS\n",
      "506_0_3000210     NAE 06 N2 1 VMA135 SUP FLOW\n",
      "                             ...             \n",
      "506_14_3002830     NAE 06 N2 2 VMA180 OCC CMD\n",
      "506_14_3002850     NAE 06 N2 2 VMA181 OCC CMD\n",
      "506_14_3002872     NAE 06 N2 2 VMA182 OCC CMD\n",
      "506_14_3002892     NAE 06 N2 2 VMA183 OCC CMD\n",
      "506_14_3002914     NAE 06 N2 2 VMA103 OCC CMD\n",
      "506_14_3002934     NAE 06 N2 2 VMA104 OCC CMD\n",
      "506_14_3002954     NAE 06 N2 2 VMA105 OCC CMD\n",
      "506_14_3002974     NAE 06 N2 2 VMA106 OCC CMD\n",
      "506_14_3002994     NAE 06 N2 2 VMA107 OCC CMD\n",
      "506_14_3003014     NAE 06 N2 2 VMA108 OCC CMD\n",
      "506_14_3003034     NAE 06 N2 2 VMA109 OCC CMD\n",
      "506_14_3003054     NAE 06 N2 2 VMA110 OCC CMD\n",
      "506_14_3003074     NAE 06 N2 2 VMA111 OCC CMD\n",
      "506_14_3003094     NAE 06 N2 2 VMA112 OCC CMD\n",
      "506_14_3003114     NAE 06 N2 2 VMA113 OCC CMD\n",
      "506_14_3003134     NAE 06 N2 2 VMA114 OCC CMD\n",
      "506_14_3003154     NAE 06 N2 2 VMA115 OCC CMD\n",
      "506_14_3003174     NAE 06 N2 2 VMA116 OCC CMD\n",
      "506_14_3003194     NAE 06 N2 2 VMA117 OCC CMD\n",
      "506_14_3003214     NAE 06 N2 2 VMA118 OCC CMD\n",
      "506_14_3003234     NAE 06 N2 2 VMA119 OCC CMD\n",
      "506_14_3003255     NAE 06 N2 2 VMA120 OCC CMD\n",
      "506_14_3003276     NAE 06 N2 2 VMA121 OCC CMD\n",
      "506_14_3003293     NAE 06 N2 2 VMA122 OCC CMD\n",
      "506_14_3003314     NAE 06 N2 2 VMA123 OCC CMD\n",
      "506_14_3003335     NAE 06 N2 2 VMA124 OCC CMD\n",
      "506_14_3003356     NAE 06 N2 2 VMA125 OCC CMD\n",
      "506_14_3003377     NAE 06 N2 2 VMA126 OCC CMD\n",
      "506_14_3009394     NAE 06 N2 1 VMA151 OCC CMD\n",
      "506_14_3011366     NAE 06 N2 2 VMA154 OCC CMD\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "subDF = sensorDF[sensorDF['name_struct']=='_0_3_1_3_0_1_3_1_3_0_1_3_0_3_0']['name']\n",
    "for row in subDF.tolist():\n",
    "    print row, '\\t', \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "oneCharSet = set()\n",
    "for colName, col in sensorDF[['description', u'jci_name', u'name']].iteritems():\n",
    "    for col.i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sensorDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4307f4b7d81f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moneCharSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mcolName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msensorDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'jci_name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mu'name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mwordList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[a-zA-Z]+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwordList\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sensorDF' is not defined"
     ]
    }
   ],
   "source": [
    "oneCharSet = set()\n",
    "for colName, col in sensorDF[['description', u'jci_name', u'name']].iteritems():\n",
    "    for sentence in col.tolist():\n",
    "        wordList = re.findall('[a-zA-Z]+', sentence)\n",
    "        for word in wordList:\n",
    "            if len(word)==1:\n",
    "                oneCharSet.add(word)\n",
    "                if word=='N':\n",
    "                    print sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
